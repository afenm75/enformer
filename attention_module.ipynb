{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMQKSMHrKWwd"
      },
      "source": [
        "**Enformer Attention Model Class: A Comprehensive Guide**\n",
        "\n",
        "The primary objective of `Enformer Attention Model` class is to facilitate the creation of `attention` models for the hidden `transformers` within `Enformers`. It has been specifically designed to handle `attention` mechanisms and their impact on `transformers`. To gain a thorough understanding of this topic, we strongly encourage you to begin by reading the accompanying report.\n",
        "\n",
        "The report provides essential insights into the concepts surrounding attention mechanisms, detailing how they function and the profound effects they have on transformers. By delving into the report, you will gain valuable knowledge about the inner workings of `attention` models and their implications within `Enformers`.\n",
        "\n",
        "By utilizing this `attention` model class, you can effectively harness the power of attention mechanisms in transformer networks. It enables you to create and manipulate attention models within the Enformer framework, offering greater control over the learning process.\n",
        "\n",
        "We highly recommend familiarizing yourself with the accompanying report to fully grasp the significance and practical implementation of attention models. If you have any questions or require further assistance, please don't hesitate to reach out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5HNL_k4q9mZ"
      },
      "source": [
        "The file defines a TransformerBlock class, which represents a full transformer module block. Here's a breakdown of the class components:\n",
        "\n",
        "-  `init() ` method: The constructor of the class. It takes the following arguments:\n",
        "  -  `channels `: The number of channels or filters in the model.\n",
        "  -  `dropout_rate `: The dropout rate used in the transformer block.\n",
        "  -  `attention_kwargs `: A dictionary containing keyword arguments for the MultiheadAttention module used in the transformer block.\n",
        "  -  `name `: The name of the module.\n",
        "\n",
        "-  `mha_ln `: An instance of  `snt.LayerNorm  `used to apply layer normalization to the multihead attention output.\n",
        "\n",
        "-  `mha `: An instance of the  `MultiheadAttention ` module with the specified attention_kwargs.\n",
        "\n",
        "-  `mha_dropout `: An instance of  `snt.Dropout ` used for dropout regularization after the multihead attention layer.\n",
        "\n",
        "-  `mlp_ln `: An instance of  `snt.LayerNorm ` used to apply layer normalization to the MLP output.\n",
        "\n",
        "-  `mlp_linear1 `: An instance of  `snt.Linear ` representing the first linear layer in the MLP.\n",
        "\n",
        "-  `mlp_dropout1 `: An instance of  `snt.Dropout ` used for dropout regularization after the first linear layer in the MLP.\n",
        "\n",
        "-  `mlp_linear2 `: An instance of  `snt.Linear ` representing the second linear layer in the MLP.\n",
        "\n",
        "-  `mlp_dropout2 `: An instance of  `snt.Dropout ` used for dropout regularization after the second linear layer in the MLP.\n",
        "\n",
        "The `TransformerBlock` class encapsulates the components required for a single transformer block, including the multihead attention layer and the feed-forward MLP layer. It applies layer normalization and dropout regularization at appropriate places within the block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi7VRsZppurY"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class TransformerBlock(snt.Module):\n",
        "  \"\"\"Full transformer module block.\"\"\"\n",
        "\n",
        "  def init(\n",
        "      self,\n",
        "      channels: int,\n",
        "      dropout_rate: float,\n",
        "      attention_kwargs: Dict[str, Any],\n",
        "      name: str = 'transformer_block',\n",
        "  ):\n",
        "    super().init(name=name)\n",
        "    self.mha_ln = snt.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
        "    self.mha = MultiheadAttention(**attention_kwargs)\n",
        "    self.mha_dropout = snt.Dropout(dropout_rate)\n",
        "\n",
        "    self.mlp_ln = snt.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
        "    self.mlp_linear1 = snt.Linear(channels * 2)\n",
        "    self.mlp_dropout1 = snt.Dropout(dropout_rate)\n",
        "    self.mlp_linear2 = snt.Linear(channels)\n",
        "    self.mlp_dropout2 = snt.Dropout(dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27hGiX5BrCao"
      },
      "source": [
        "The  `call()` method of the  `TransformerBlock ` class implements the forward pass of the transformer block. It takes the following arguments:\n",
        "\n",
        "-  `inputs `: The input tensor to the transformer block.\n",
        "-  `is_training `: A boolean flag indicating whether the model is in training mode or not.\n",
        "\n",
        "Here's a breakdown of the steps performed in the  `call() ` method:\n",
        "\n",
        "1. Apply layer normalization  `(self.mha_ln) ` to the input tensor.\n",
        "2. Pass the normalized tensor through the multihead attention layer  `(self.mha) ` with the specified  `is_training ` flag.\n",
        "3. Apply dropout regularization  `(self.mha_dropout) ` to the output of the multihead attention layer.\n",
        "4. Add the residual connection by adding the input tensor to the output of the multihead attention layer.\n",
        "5. Store the output of the multihead attention layer in the variable `mha_output`.\n",
        "6. Apply layer normalization  `(self.mlp_ln) ` to the  `mha_output `.\n",
        "7. Pass the normalized tensor through the first linear layer in the MLP  `(self.mlp_linear1) `.\n",
        "8. Apply dropout regularization  `(self.mlp_dropout1) ` to the output of the first linear layer.\n",
        "9. Apply the  `ReLU ` activation function to the output of the first linear layer.\n",
        "10. Pass the  `ReLU-activated ` tensor through the second linear layer in the MLP  `(self.mlp_linear2) `.\n",
        "11. Apply  `dropout ` regularization  `(self.mlp_dropout2) ` to the output of the second linear layer.\n",
        "12. Add the output of the MLP to the  `mha_output (residual connection) `.\n",
        "13. Return the final output tensor.\n",
        "\n",
        "The  `call()`method applies layer normalization,  `multihead ` attention, and  `feed-forward MLP ` operations to the input tensor, while maintaining residual connections between the different operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqyMg_M3rF0o"
      },
      "outputs": [],
      "source": [
        "def call(self, inputs: tf.Tensor, is_training: bool) -> tf.Tensor:\n",
        "    x = self.mha_ln(inputs)\n",
        "    x = self.mha(x, is_training=is_training)\n",
        "    x = self.mha_dropout(x, is_training=is_training)\n",
        "    x += inputs  # Residual\n",
        "    mha_output = x\n",
        "\n",
        "    # MLP.\n",
        "    x = self.mlp_ln(mha_output)\n",
        "    x = self.mlp_linear1(x)\n",
        "    x = self.mlp_dropout1(x, is_training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.mlp_linear2(x)\n",
        "    x = self.mlp_dropout2(x, is_training=is_training)\n",
        "    return x + mha_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiTLxd4QrKvZ"
      },
      "source": [
        "The `MultiheadAttention` class is a module that implements the multi-head attention mechanism used in transformers. It has the following key features:\n",
        "\n",
        "- It supports both absolute and relative positional encodings.\n",
        "- It allows for different types of relative positional biases through the  `relative_position_functions ` argument.\n",
        "- It provides options for scaling the attention logits, applying dropout to attention logits, and zero initialization of the final linear layer.\n",
        "- The module consists of linear projection layers for  `keys `,  `queries `, and  `values `, as well as an  `embedding layer `.\n",
        "- If relative positions are used, additional linear projection layers and biases are created.\n",
        "\n",
        "Here's a breakdown of the main components and their purposes:\n",
        "\n",
        "-  `value_size `: The size of each value embedding per head.\n",
        "-  `key_size `: The size of each key and query embedding per head.\n",
        "-  `num_heads `: The number of independent queries per timestep.\n",
        "-  `scaling `: Whether to scale the attention logits.\n",
        "-  `attention_dropout_rate `: Dropout rate for attention logits.\n",
        "-  `relative_positions `: Whether to use TransformerXL style relative attention.\n",
        "-  `relative_position_symmetric `: If `True`, the symmetric version of basis functions will be used. If `False`, both symmetric and asymmetric versions will be used.\n",
        "-  `relative_position_functions() `: List of function names used for relative positional biases.\n",
        "-  `num_relative_position_features `: Number of relative positional features to compute. If None, it defaults to `value_size * num_heads`.\n",
        "-  `positional_dropout_rate `: Dropout rate for the positional encodings if relative positions are used.\n",
        "-  `zero_initialize `: If `True`, the final linear layer will be initialized with zeros.\n",
        "-  `initializer `: Initializer for the projection layers. If not specified, `VarianceScaling` is used with a scale of `2.0`.\n",
        "\n",
        "The module consists of several linear projection layers  `(_q_layer, _k_layer, _v_layer, _embedding_layer, _r_k_layer) ` with specified sizes and initializers. The projection layers are used to project the input tensor into the appropriate dimensions for multi-head attention calculations.\n",
        "\n",
        "If relative positions are used  `(_relative_positions=True) `, additional linear projection layers  `(_r_k_layer) ` and biases  `(_r_w_bias, _r_r_bias)  `are created to handle relative positional encodings.\n",
        "\n",
        "The module implements the  `call()` method, which performs the forward pass of the multi-head attention mechanism. It takes an input tensor and returns the output tensor after applying the multi-head attention operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N55sb8_0rLwS"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(snt.Module):\n",
        "  \"\"\"Multi-head attention.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               value_size: int,\n",
        "               key_size: int,\n",
        "               num_heads: int,\n",
        "               scaling: bool = True,\n",
        "               attention_dropout_rate: float = 0.1,\n",
        "               relative_positions: bool = False,\n",
        "               relative_position_symmetric: bool = False,\n",
        "               relative_position_functions: Optional[List[str]] = None,\n",
        "               num_relative_position_features: Optional[int] = None,\n",
        "               positional_dropout_rate: float = 0.1,\n",
        "               zero_initialize: bool = True,\n",
        "               initializer: Optional[snt.initializers.Initializer] = None,\n",
        "               name: str = None):\n",
        "    \"\"\"Creates a MultiheadAttention module.\n",
        "\n",
        "    Args:\n",
        "      value_size: The size of each value embedding per head.\n",
        "      key_size: The size of each key and query embedding per head.\n",
        "      num_heads: The number of independent queries per timestep.\n",
        "      scaling: Whether to scale the attention logits.\n",
        "      attention_dropout_rate: Dropout rate for attention logits.\n",
        "      relative_positions: Whether to use TransformerXL style relative attention.\n",
        "      relative_position_symmetric: If True, the symmetric version of basis\n",
        "        functions will be used. If False, a symmetric and asymmetric versions\n",
        "        will be use.\n",
        "      relative_position_functions: List of function names used for relative\n",
        "        positional biases.\n",
        "      num_relative_position_features: Number of relative positional features\n",
        "        to compute. If None, `value_size * num_heads` is used.\n",
        "      positional_dropout_rate: Dropout rate for the positional encodings if\n",
        "        relative positions are used.\n",
        "      zero_initialize: if True, the final linear layer will be 0 initialized.\n",
        "      initializer: Initializer for the projection layers. If unspecified,\n",
        "        VarianceScaling is used with scale = 2.0.\n",
        "      name: Name of module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self._value_size = value_size\n",
        "    self._key_size = key_size\n",
        "    self._num_heads = num_heads\n",
        "    self._attention_dropout_rate = attention_dropout_rate\n",
        "    self._scaling = scaling\n",
        "    self._relative_positions = relative_positions\n",
        "    self._relative_position_symmetric = relative_position_symmetric\n",
        "    self._relative_position_functions = relative_position_functions\n",
        "    if num_relative_position_features is None:\n",
        "      # num_relative_position_features needs to be divisible by the number of\n",
        "      # relative positional functions *2 (for symmetric & asymmetric version).\n",
        "      divisible_by = 2 * len(self._relative_position_functions)\n",
        "      self._num_relative_position_features = (\n",
        "          (self._value_size // divisible_by) * divisible_by)\n",
        "    else:\n",
        "      self._num_relative_position_features = num_relative_position_features\n",
        "    self._positional_dropout_rate = positional_dropout_rate\n",
        "\n",
        "    self._initializer = initializer\n",
        "    if self._initializer is None:\n",
        "      self._initializer = snt.initializers.VarianceScaling(scale=2.0)\n",
        "\n",
        "    key_proj_size = self._key_size * self._num_heads\n",
        "    embedding_size = self._value_size * self._num_heads\n",
        "\n",
        "    self._q_layer = snt.Linear(\n",
        "        key_proj_size,\n",
        "        name='q_layer',\n",
        "        with_bias=False,\n",
        "        w_init=self._initializer)\n",
        "    self._k_layer = snt.Linear(\n",
        "        key_proj_size,\n",
        "        name='k_layer',\n",
        "        with_bias=False,\n",
        "        w_init=self._initializer)\n",
        "    self._v_layer = snt.Linear(\n",
        "        embedding_size,\n",
        "        name='v_layer',\n",
        "        with_bias=False,\n",
        "        w_init=self._initializer)\n",
        "    w_init = snt.initializers.Zeros() if zero_initialize else self._initializer\n",
        "    self._embedding_layer = snt.Linear(\n",
        "        embedding_size,\n",
        "        name='embedding_layer',\n",
        "        w_init=w_init)\n",
        "\n",
        "    # Create additional layers if using relative positions.\n",
        "    if self._relative_positions:\n",
        "      self._r_k_layer = snt.Linear(\n",
        "          key_proj_size,\n",
        "          name='r_k_layer',\n",
        "          with_bias=False,\n",
        "          w_init=self._initializer)\n",
        "      self._r_w_bias = tf.Variable(\n",
        "          self._initializer([1, self._num_heads, 1, self._key_size],\n",
        "                            dtype=tf.float32),\n",
        "          name='r_w_bias')\n",
        "      self._r_r_bias = tf.Variable(\n",
        "          self._initializer([1, self._num_heads, 1, self._key_size],\n",
        "                            dtype=tf.float32),\n",
        "          name='r_r_bias')\n",
        "\n",
        "  def _multihead_output(self, linear, inputs):\n",
        "    \"\"\"Applies a standard linear to inputs and returns multihead output.\"\"\"\n",
        "\n",
        "    output = snt.BatchApply(linear)(inputs)  # [B, T, H * KV]\n",
        "    num_kv_channels = output.shape[-1] // self._num_heads\n",
        "    # Split H * Channels into separate axes.\n",
        "    output = snt.reshape(output,\n",
        "                         output_shape=[-1, self._num_heads, num_kv_channels])\n",
        "    # [B, T, H, KV] -> [B, H, T, KV]\n",
        "    return tf.transpose(output, [0, 2, 1, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFFkZQCUr30h"
      },
      "source": [
        "The  `call()`method of the `MultiheadAttention` class implements the forward pass of the multi-head attention mechanism. Here's a breakdown of the steps performed in the method:\n",
        "\n",
        "1. Initialize the projection layers and compute the dimensions.\n",
        "2. Compute the `queries`, `keys`, and `values` by applying multi-headed projections of the inputs using the `_multihead_output` method.\n",
        "3. Scale the queries by the square root of the key size if scaling is enabled.\n",
        "4. If relative positions are enabled, compute the positional encodings and project them to form relative keys  `(r_k) `.\n",
        "5. Compute the logits by performing matrix multiplication between queries and keys.\n",
        "   - If relative positions are enabled, add the shifted relative logits to the content logits.\n",
        "6. Apply  `softmax ` to obtain attention weights.\n",
        "7. Apply  `dropout ` to the attention weights if in training mode.\n",
        "8. Compute the attended inputs by multiplying the attention weights with values.\n",
        "9. Transpose and reshape the output to the desired shape.\n",
        "10. Apply a final linear layer to obtain the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R8rA_AVsCqy"
      },
      "outputs": [],
      "source": [
        "def _multihead_output(self, linear, inputs):\n",
        "    \"\"\"Applies a standard linear to inputs and returns multihead output.\"\"\"\n",
        "\n",
        "    output = snt.BatchApply(linear)(inputs)  # [B, T, H * KV]\n",
        "    num_kv_channels = output.shape[-1] // self._num_heads\n",
        "    # Split H * Channels into separate axes.\n",
        "    output = snt.reshape(output,\n",
        "                         output_shape=[-1, self._num_heads, num_kv_channels])\n",
        "    # [B, T, H, KV] -> [B, H, T, KV]\n",
        "    return tf.transpose(output, [0, 2, 1, 3])\n",
        "\n",
        "  def call(self,\n",
        "               inputs,\n",
        "               is_training=False):\n",
        "    # Initialise the projection layers.\n",
        "    embedding_size = self._value_size * self._num_heads\n",
        "    seq_len = inputs.shape[1]\n",
        "\n",
        "    # Compute q, k and v as multi-headed projections of the inputs.\n",
        "    q = self._multihead_output(self._q_layer, inputs)  # [B, H, T, K]\n",
        "    k = self._multihead_output(self._k_layer, inputs)  # [B, H, T, K]\n",
        "    v = self._multihead_output(self._v_layer, inputs)  # [B, H, T, V]\n",
        "\n",
        "    # Scale the query by the square-root of key size.\n",
        "    if self._scaling:\n",
        "      q *= self._key_size**-0.5\n",
        "\n",
        "    if self._relative_positions:\n",
        "      # For relative positions, we project positions to form relative keys.\n",
        "      distances = tf.range(-seq_len + 1, seq_len, dtype=tf.float32)[tf.newaxis]\n",
        "      positional_encodings = positional_features_all(\n",
        "          positions=distances,\n",
        "          feature_size=self._num_relative_position_features,\n",
        "          seq_length=seq_len,\n",
        "          feature_functions=self._relative_position_functions,\n",
        "          symmetric=self._relative_position_symmetric)\n",
        "      # [1, 2T-1, Cr]\n",
        "\n",
        "      if is_training:\n",
        "        positional_encodings = tf.nn.dropout(\n",
        "            positional_encodings, rate=self._positional_dropout_rate)\n",
        "\n",
        "      # [1, H, 2T-1, K]\n",
        "      r_k = self._multihead_output(self._r_k_layer, positional_encodings)\n",
        "\n",
        "      # Add shifted relative logits to content logits.\n",
        "      # [B, H, T', T]\n",
        "      content_logits = tf.matmul(q + self._r_w_bias, k, transpose_b=True)\n",
        "      # [B, H, T', 2T-1]\n",
        "      relative_logits = tf.matmul(\n",
        "          q + self._r_r_bias, r_k, transpose_b=True)\n",
        "      #  [B, H, T', T]\n",
        "      relative_logits = relative_shift(relative_logits)\n",
        "      logits = content_logits + relative_logits\n",
        "    else:\n",
        "      # [B, H, T', T]\n",
        "      logits = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    weights = tf.nn.softmax(logits)\n",
        "\n",
        "    # Dropout on the attention weights.\n",
        "    if is_training:\n",
        "      weights = tf.nn.dropout(weights, rate=self._attention_dropout_rate)\n",
        "\n",
        "    # Transpose and reshape the output.\n",
        "    output = tf.matmul(weights, v)  # [B, H, T', V]\n",
        "    output_transpose = tf.transpose(output, [0, 2, 1, 3])  # [B, T', H, V]\n",
        "    # Final linear layer.\n",
        "    attended_inputs = snt.reshape(\n",
        "        output_transpose, output_shape=[embedding_size], preserve_dims=2)\n",
        "    output = self._embedding_layer(attended_inputs)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14HMBN-usN8s"
      },
      "source": [
        "Now the author includes additional `helper()` functions for handling relative positions and computing positional encodings/features. Here's an explanation of these functions:\n",
        "\n",
        "1.  `relative_shift() `: This function is used to shift the relative `logits`, following the approach used in  `TransformerXL `. It prepends zeros to the final timescale dimension of the input `x`, then performs reshaping and slicing operations to obtain the shifted logits.\n",
        "\n",
        "2.  `get_positional_feature_function() `: This function returns the positional feature functions based on the provided name. It maintains a dictionary of available feature functions and raises an error if an invalid function name is provided.\n",
        "\n",
        "3.  `positional_features_all() `: This function computes the relative positional encodings/features. It takes various arguments such as  `positions `  (tensor of relative positions),  `feature_size ` (total number of basis functions),  `seq_length ` (characteristic length of individual positional features),  `bin_size ` (bin size used for partitioning the sequence),  `feature_functions ` (list of feature function names), and  `symmetric ` (boolean indicating whether the resulting features should be symmetric).\n",
        "\n",
        "   The function iterates over the `feature_functions`, retrieves the corresponding function using  `get_positional_feature_function `, and applies it to compute positional features based on the given parameters. The resulting features are concatenated along the feature axis. If symmetric is  `True `, the features are symmetrically  duplicated across the relative position of  `0 `. The final tensor shape is checked for compatibility before returning the embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwGQNT2PsO2K"
      },
      "outputs": [],
      "source": [
        "def relative_shift(x):\n",
        "  \"\"\"Shift the relative logits like in TransformerXL.\"\"\"\n",
        "  # We prepend zeros on the final timescale dimension.\n",
        "  to_pad = tf.zeros_like(x[..., :1])\n",
        "  x = tf.concat([to_pad, x], -1)\n",
        "  _, num_heads, t1, t2 = x.shape\n",
        "  x = tf.reshape(x, [-1, num_heads, t2, t1])\n",
        "  x = tf.slice(x, [0, 0, 1, 0], [-1, -1, -1, -1])\n",
        "  x = tf.reshape(x, [-1, num_heads, t1, t2 - 1])\n",
        "  x = tf.slice(x, [0, 0, 0, 0], [-1, -1, -1, (t2 + 1) // 2])\n",
        "  return x\n",
        "\n",
        "\n",
        "# Available feature functions:\n",
        "def get_positional_feature_function(name):\n",
        "  \"\"\"Returns positional feature functions.\"\"\"\n",
        "  available = {\n",
        "      'positional_features_exponential': positional_features_exponential,\n",
        "      'positional_features_central_mask': positional_features_central_mask,\n",
        "      'positional_features_gamma': positional_features_gamma,\n",
        "      'positional_features_cosine': positional_features_cosine,\n",
        "      'positional_features_linear_masks': positional_features_linear_masks,\n",
        "      'positional_features_sin_cos': positional_features_sin_cos,\n",
        "  }\n",
        "  if name not in available:\n",
        "    raise ValueError(f'Function {name} not available in {available.keys()}')\n",
        "  return available[name]\n",
        "\n",
        "\n",
        "def positional_features_all(positions: tf.Tensor,\n",
        "                            feature_size: int,\n",
        "                            seq_length: Optional[int] = None,\n",
        "                            bin_size: Optional[int] = None,\n",
        "                            feature_functions: Optional[List[str]] = None,\n",
        "                            symmetric=False):\n",
        "  \"\"\"Compute relative positional encodings/features.\n",
        "\n",
        "  Each positional feature function will compute/provide the same fraction of\n",
        "  features, making up the total of feature_size.\n",
        "  Args:\n",
        "    positions: Tensor of relative positions of arbitrary shape.\n",
        "    feature_size: Total number of basis functions.\n",
        "    seq_length: Sequence length denoting the characteristic length that\n",
        "      the individual positional features can use. This is required since the\n",
        "      parametrization of the input features should be independent of positions\n",
        "      while it could still require to use the total number of features.\n",
        "    bin_size: Bin sized used to partition the sequence. This can be used to\n",
        "      compute features on the absolute scale relative to the genome.\n",
        "    feature_functions: List of different feature functions to use. Each function\n",
        "      will take as argument: positions, sequence length and number of features\n",
        "      to compute.\n",
        "    symmetric: If True, the resulting features will be symmetric across the\n",
        "      relative position of 0 (i.e. only absolute value of positions will\n",
        "      matter). If false, then both the symmetric and asymmetric version\n",
        "      (symmetric multiplied by sign(positions)) of the features will be used.\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape: positions.shape + (feature_size,).\n",
        "  \"\"\"\n",
        "  if feature_functions is None:\n",
        "    feature_functions = ['positional_features_exponential',\n",
        "                         'positional_features_central_mask',\n",
        "                         'positional_features_gamma']\n",
        "  num_components = len(feature_functions)  # 1 per each basis function\n",
        "  if not symmetric:\n",
        "    num_components = 2 * num_components\n",
        "\n",
        "  # For now, we do not allow odd sized embeddings.\n",
        "  if feature_size % num_components != 0:\n",
        "    raise ValueError(\n",
        "        f'feature_size has to be divisible by {num_components}')\n",
        "\n",
        "  feature_functions = [get_positional_feature_function(f)\n",
        "                       for f in feature_functions]\n",
        "  num_basis_per_class = feature_size // num_components\n",
        "  embeddings = tf.concat([f(tf.abs(positions), num_basis_per_class,\n",
        "                            seq_length, bin_size)\n",
        "                          for f in feature_functions],\n",
        "                         axis=-1)\n",
        "  if not symmetric:\n",
        "    embeddings = tf.concat([embeddings,\n",
        "                            tf.sign(positions)[..., tf.newaxis] * embeddings],\n",
        "                           axis=-1)\n",
        "  tf.TensorShape(embeddings.shape).assert_is_compatible_with(\n",
        "      positions.shape + [feature_size])\n",
        "  return embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtK0NsrHsVR7"
      },
      "source": [
        "These are the additional positional feature functions:\n",
        "\n",
        "1.  `_prepend_dims() `: This function is a utility function that prepends dimensions to a tensor. It reshapes the tensor by adding `num_dims` dimensions with size `1` at the beginning.\n",
        "\n",
        "2.  `positional_features_exponential() `: This function creates exponentially decaying positional weights. It takes positions (position tensor),  `feature_size  `(number of basis functions),  `seq_length ` (sequence length),  `bin_size ` (unused), and  `min_half_life ` (smallest exponential half-life) as arguments. If `seq_length` is not provided, it is computed as the maximum absolute value of positions plus 1.\n",
        "\n",
        "   The function calculates a grid of half-lives from `min_half_life` to `max_range` on a logarithmic scale, where `max_range` is determined based on `seq_length`. The `half-lives` are used to compute exponential weights for each position in `positions`. The outputs are shaped as  `[2 * seq_length - 1, feature_size] `, representing the positional features.\n",
        "\n",
        "3.  `positional_features_central_mask() `: This function creates positional features using a central mask, allowing only central features. It takes `positions` (position tensor), `feature_size` (number of basis functions), `seq_length` (unused), and `bin_size` (unused) as arguments.\n",
        "\n",
        "   The function generates a series of center widths using powers of 2, and each width is adjusted by subtracting 1. It then checks if the absolute positions are within the corresponding center widths and converts the result into a float tensor. The outputs have a shape of  `[positions.shape + [feature_size]] `.\n",
        "\n",
        "4.  `gamma_pdf() `: This function computes the probability density function (PDF) of the gamma distribution. It takes  `x ` (input values), concentration, and rate as arguments and returns the PDF values. The function uses the gamma distribution formula to calculate the logarithm of the unnormalized probability and the logarithm of the normalization constant. Finally, it exponentiates the difference to obtain the PDF values.\n",
        "\n",
        "These functions provide different strategies for generating positional encodings/features based on the positions and other parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le37qQ-Ksb6e"
      },
      "outputs": [],
      "source": [
        "def _prepend_dims(x, num_dims):\n",
        "  return tf.reshape(x, shape=[1] * num_dims + x.shape)\n",
        "\n",
        "\n",
        "def positional_features_exponential(positions: tf.Tensor,\n",
        "                                    feature_size: int,\n",
        "                                    seq_length: Optional[int] = None,\n",
        "                                    bin_size: Optional[int] = None,\n",
        "                                    min_half_life: Optional[float] = 3.0):\n",
        "  \"\"\"Create exponentially decaying positional weights.\n",
        "\n",
        "  Args:\n",
        "    positions: Position tensor (arbitrary shape).\n",
        "    feature_size: Number of basis functions to use.\n",
        "    seq_length: Sequence length.\n",
        "    bin_size: (unused). See positional_features_all.\n",
        "    min_half_life: Smallest exponential half life in the grid of half lives.\n",
        "\n",
        "  Returns:\n",
        "    A Tensor with shape [2 * seq_length - 1, feature_size].\n",
        "  \"\"\"\n",
        "  del bin_size  # Unused.\n",
        "  if seq_length is None:\n",
        "    seq_length = tf.reduce_max(tf.abs(positions)) + 1\n",
        "  # Grid of half lifes from [3, seq_length / 2] with feature_size\n",
        "  # distributed on the log scale.\n",
        "  seq_length = tf.cast(seq_length, dtype=tf.float32)\n",
        "  max_range = tf.math.log(seq_length) / tf.math.log(2.0)\n",
        "  half_life = tf.pow(2.0, tf.linspace(min_half_life, max_range, feature_size))\n",
        "  half_life = _prepend_dims(half_life, positions.shape.rank)\n",
        "  positions = tf.abs(positions)\n",
        "  outputs = tf.exp(-tf.math.log(2.0) / half_life * positions[..., tf.newaxis])\n",
        "  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n",
        "      positions.shape + [feature_size])\n",
        "  return outputs\n",
        "  def positional_features_central_mask(positions: tf.Tensor,\n",
        "                                     feature_size: int,\n",
        "                                     seq_length: Optional[int] = None,\n",
        "                                     bin_size: Optional[int] = None):\n",
        "  \"\"\"Positional features using a central mask (allow only central features).\"\"\"\n",
        "  del seq_length  # Unused.\n",
        "  del bin_size  # Unused.\n",
        "  center_widths = tf.pow(2.0, tf.range(1, feature_size + 1, dtype=tf.float32))\n",
        "  center_widths = center_widths - 1\n",
        "  center_widths = _prepend_dims(center_widths, positions.shape.rank)\n",
        "  outputs = tf.cast(center_widths > tf.abs(positions)[..., tf.newaxis],\n",
        "                    tf.float32)\n",
        "  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n",
        "      positions.shape + [feature_size])\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def gamma_pdf(x, concentration, rate):\n",
        "  \"\"\"Gamma probability distribution function: p(x|concentration, rate).\"\"\"\n",
        "  log_unnormalized_prob = tf.math.xlogy(concentration - 1., x) - rate * x\n",
        "  log_normalization = (tf.math.lgamma(concentration) -\n",
        "                       concentration * tf.math.log(rate))\n",
        "  return tf.exp(log_unnormalized_prob - log_normalization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGgU5x8SsfTM"
      },
      "source": [
        "Here are two more positional feature functions:\n",
        "\n",
        "1.  `positional_features_gamma() `: This function computes positional features using gamma distributions. It takes  `positions ` (position tensor),  `feature_size ` (number of basis functions),  `seq_length ` (sequence length),  `bin_size  `(unused), stddev (standard deviation), and  `start_mean ` (starting mean) as arguments.\n",
        "\n",
        "   The function first computes the sequence length if it is not provided by taking the maximum absolute value of positions and adding 1. It then calculates the mean values for each basis function on a linear scale from `start_mean` to `seq_length`. The concentration and rate parameters of the gamma distribution are computed based on the mean and stddev values. The probabilities are obtained by evaluating the gamma PDF at the absolute positions. A small constant is added to ensure numerical stability, and the probabilities are normalized by dividing them by the maximum probability along the feature dimension. The resulting outputs have a shape of [positions.shape + [feature_size]].\n",
        "\n",
        "2.  `positional_features_cosine() `: This function generates cosine positional features. It takes  `positions ` (position tensor),  `feature_size ` (number of basis functions),  `seq_length ` (unused), and  `bin_size ` (unused) as arguments.\n",
        "\n",
        "   The function defines a periodicity value for each basis function based on a geometric series. The cosine of the positions divided by the corresponding periodicity values is computed to generate the cosine positional features. The outputs have a shape of [positions.shape + [feature_size]].\n",
        "\n",
        "These functions provide additional options for generating positional encodings/features based on gamma distributions and cosine functions, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_0GpHgesiL2"
      },
      "outputs": [],
      "source": [
        "def positional_features_gamma(positions: tf.Tensor,\n",
        "                              feature_size: int,\n",
        "                              seq_length: Optional[int] = None,\n",
        "                              bin_size: Optional[int] = None,\n",
        "                              stddev=None,\n",
        "                              start_mean=None):\n",
        "  \"\"\"Positional features computed using the gamma distributions.\"\"\"\n",
        "  del bin_size  # Unused.\n",
        "  if seq_length is None:\n",
        "    seq_length = tf.reduce_max(tf.abs(positions)) + 1\n",
        "  if stddev is None:\n",
        "    stddev = seq_length / (2 * feature_size)\n",
        "  if start_mean is None:\n",
        "    start_mean = seq_length / feature_size\n",
        "  mean = tf.linspace(start_mean, seq_length, num=feature_size)\n",
        "  mean = _prepend_dims(mean, positions.shape.rank)\n",
        "  concentration = (mean / stddev)**2\n",
        "  rate = mean / stddev**2\n",
        "  probabilities = gamma_pdf(\n",
        "      tf.abs(tf.cast(positions, dtype=tf.float32))[..., tf.newaxis],\n",
        "      concentration, rate)\n",
        "  probabilities += 1e-8  # To ensure numerical stability.\n",
        "  outputs = probabilities / tf.reduce_max(probabilities,\n",
        "                                          axis=1, keepdims=True)\n",
        "  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n",
        "      positions.shape + [feature_size])\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def positional_features_cosine(positions: tf.Tensor,\n",
        "                               feature_size: int,\n",
        "                               seq_length: Optional[int] = None,\n",
        "                               bin_size: Optional[int] = None):\n",
        "  \"\"\"Cosine positional features.\"\"\"\n",
        "  del bin_size  # Unused.\n",
        "  del seq_length  # Unused.\n",
        "  periodicity = 1.25 * tf.pow(2.0, tf.range(0, feature_size, dtype=tf.float32))\n",
        "  periodicity = _prepend_dims(periodicity, positions.shape.rank)\n",
        "\n",
        "  outputs = tf.math.cos(2 * np.pi * positions[..., tf.newaxis] / periodicity)\n",
        "  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n",
        "      positions.shape + [feature_size])\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJAIS5husmMp"
      },
      "source": [
        "Here are two more positional feature functions:\n",
        "\n",
        "1.  `positional_features_linear_masks() `: This function generates exponentially increasing point focuses. It takes  `positions ` (position tensor),  `feature_size  `(number of basis functions),  `seq_length ` (unused), and  `bin_size ` (unused) as arguments.\n",
        "\n",
        "   The function creates a range of distances from 0  to  `feature_size ` - 1. It then compares each distance with the absolute positions and generates a binary mask where the mask value is 1 if the distance matches the absolute position and 0 otherwise. The outputs have a shape of  `[positions.shape + [feature_size]] `.\n",
        "\n",
        "2.  `positional_features_sin_cos() `: This function generates sine/cosine positional encodings. It takes  `positions ` (position tensor),  `feature_size ` (number of basis functions),  `seq_length ` (unused),  `bin_size ` (unused), and  `max_time ` (maximum time value) as arguments.\n",
        "\n",
        "   The function first checks if `feature_size` is divisible by 2. It then creates a range of values `i` from 0 to `feature_size - 1` with a `step` of 2. Sine and cosine positional encodings are computed by dividing the positions by `max_time` raised to the power of `(i / feature_size)`. The sine and cosine values are concatenated along the last dimension to form the outputs, which have a shape of `[positions.shape + [feature_size]]`.\n",
        "\n",
        "These functions provide additional options for generating positional encodings/features based on linear masks and sine/cosine functions, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9XaEBKxspVK"
      },
      "outputs": [],
      "source": [
        "def positional_features_linear_masks(positions: tf.Tensor,\n",
        "                                     feature_size: int,\n",
        "                                     seq_length: Optional[int] = None,\n",
        "                                     bin_size: Optional[int] = None):\n",
        "  \"\"\"Exponentially increasing point focuses.\"\"\"\n",
        "  del bin_size  # Unused.\n",
        "  del seq_length  # Unused.\n",
        "  distances = tf.range(0, feature_size, dtype=tf.float32)\n",
        "  distances = _prepend_dims(distances, positions.shape.rank)\n",
        "  outputs = tf.cast(distances == tf.abs(positions[..., tf.newaxis]),\n",
        "                    dtype=tf.float32)\n",
        "\n",
        "  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n",
        "      positions.shape + [feature_size])\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def positional_features_sin_cos(positions: tf.Tensor,\n",
        "                                feature_size: int,\n",
        "                                seq_length: Optional[int] = None,\n",
        "                                bin_size: Optional[int] = None,\n",
        "                                max_time=10000.0):\n",
        "  \"\"\"Sine/cosine positional encodings.\"\"\"\n",
        "  del bin_size  # Unused.\n",
        "  del seq_length  # Unused.\n",
        "  if feature_size % 2 != 0:\n",
        "    raise ValueError('feature_size needs to be divisible by 2.')\n",
        "  i = tf.range(0, feature_size, 2, dtype=tf.float32)\n",
        "  i = _prepend_dims(i, positions.shape.rank)\n",
        "\n",
        "  # Concat sines and cosines and return.\n",
        "  outputs = tf.concat([\n",
        "      tf.sin(positions[..., tf.newaxis] / max_time**(i / feature_size)),\n",
        "      tf.cos(positions[..., tf.newaxis] / max_time**(i / feature_size))], -1)\n",
        "\n",
        "  tf.TensorShape(outputs.shape).assert_is_compatible_with(\n",
        "      positions.shape + [feature_size])\n",
        "  return outputs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}