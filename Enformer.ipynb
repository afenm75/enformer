{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTVv9c6CCsy"
      },
      "source": [
        "**Enformer Model Training Class: A Guide**\n",
        "\n",
        "This `class` is designed specifically for training  `Enformer ` models. We highly recommend that you start by reading the accompanying report, as it introduces important concepts related to Enformer training.\n",
        "\n",
        "The  `Enformer `  `training` class follows the principles of object-oriented programming. Its main purpose in this class is to handle data processing and preparation for the training phase. By utilizing this class, you can effectively manage data input and streamline the training process.\n",
        "\n",
        "By leveraging the power of object-oriented programming, this class provides a structured and efficient approach to Enformer model training. It ensures that data is properly handled and prepared for optimal training outcomes.\n",
        "\n",
        "For a deeper understanding of the Enformer model and its training process, please refer to the provided report. If you have any questions or need further assistance, feel free to reach out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD-chotCnOVO"
      },
      "source": [
        "This code includes the following imports and global variables:\n",
        "\n",
        "Imports:\n",
        "-  `inspect `: A module that provides several useful functions for inspecting live objects, such as modules, classes, methods, functions, etc.\n",
        "-  `typing `: A module that provides support for type hints.\n",
        "-  `attention_module `: It seems to be a custom module or a module from a third-party library that is imported and used in the code.\n",
        "-  `numpy (np alias) `: A popular library for numerical computing with  `Python `.\n",
        "-  `sonnet (snt alias) `: A neural network library built on top of  `TensorFlow `.\n",
        "\n",
        "Global Variables:\n",
        "-  `SEQUENCE_LENGTH `: An integer constant with the value  `196,608 `.\n",
        "-  `BIN_SIZE `: An integer constant with the value  `128 `.\n",
        "-  `TARGET_LENGTH `: An integer constant with the value  `896 `.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvK3P8uVnD84"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tensorflow implementation of Enformer model.\n",
        "\n",
        "\"Effective gene expression prediction from sequence by integrating long-range\n",
        "interactions\"\n",
        "\n",
        "Å½iga Avsec1, Vikram Agarwal2,4, Daniel Visentin1,4, Joseph R. Ledsam1,3,\n",
        "Agnieszka Grabska-Barwinska1, Kyle R. Taylor1, Yannis Assael1, John Jumper1,\n",
        "Pushmeet Kohli1, David R. Kelley2*\n",
        "\n",
        "1 DeepMind, London, UK\n",
        "2 Calico Life Sciences, South San Francisco, CA, USA\n",
        "3 Google, Tokyo, Japan\n",
        "4 These authors contributed equally.\n",
        "* correspondence: avsec@google.com, pushmeet@google.com, drk@calicolabs.com\n",
        "\"\"\"\n",
        "import inspect\n",
        "from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
        "\n",
        "import attention_module\n",
        "import numpy as np\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "\n",
        "SEQUENCE_LENGTH = 196_608\n",
        "BIN_SIZE = 128\n",
        "TARGET_LENGTH = 896"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dT1wZernVX8"
      },
      "source": [
        "The author defines a class Enformer that inherits from snt.Module (Sonnet module) and represents the main model. Here's a breakdown of the code:\n",
        "\n",
        "- The  `init() ` method initializes the `Enformer` object with several parameters:\n",
        "  -  `channels `: Number of convolutional filters and the overall width of the model.\n",
        "  -  `num_transformer_layers `: Number of transformer layers.\n",
        "  -  `num_heads `: Number of attention heads.\n",
        "  -  `pooling_type `: Specifies the pooling function to use  `('attention' or 'max') `.\n",
        "  -  `name `: Name of the `Sonnet` module.\n",
        "\n",
        "- Inside the  `init() ` method, various settings and parameters are defined for the Enformer model:\n",
        "  -  `heads_channels `: A dictionary mapping specific organisms  `('human', 'mouse') ` to the number of channels.\n",
        "  -  `dropout_rate `: The dropout rate used in the model.\n",
        "  -  `whole_attention_kwargs `: A dictionary containing various settings for the attention mechanism in the model, such as dropout rate, initializer, key size, positional dropout rate, etc.\n",
        "\n",
        "- The code then defines a name scope using  `tf.name_scope('trunk') `, which is entered using  `__enter__() `.\n",
        "\n",
        "- A  `helper ` function  `conv_block ` is defined, which constructs a convolutional block consisting of cross-replica batch normalization,  `GELU activation `, and  `1D convolution `.\n",
        "\n",
        "- The `stem` module is defined as a sequential composition of operations using the Sequential class from Sonnet. It consists of a 1D convolution, a  `residual block `, and a pooling operation.\n",
        "\n",
        "- The  `conv_tower ` module is defined as a sequential composition of several convolutional tower blocks. Each block in the tower consists of a convolutional block, a residual block, and a pooling operation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo-Kmzx6nh_T"
      },
      "outputs": [],
      "source": [
        "class Enformer(snt.Module):\n",
        "  \"\"\"Main model.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               channels: int = 1536,\n",
        "               num_transformer_layers: int = 11,\n",
        "               num_heads: int = 8,\n",
        "               pooling_type: str = 'attention',\n",
        "               name: str = 'enformer'):\n",
        "    \"\"\"Enformer model.\n",
        "\n",
        "    Args:\n",
        "      channels: Number of convolutional filters and the overall 'width' of the\n",
        "        model.\n",
        "      num_transformer_layers: Number of transformer layers.\n",
        "      num_heads: Number of attention heads.\n",
        "      pooling_type: Which pooling function to use. Options: 'attention' or max'.\n",
        "      name: Name of sonnet module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    # pylint: disable=g-complex-comprehension,g-long-lambda,cell-var-from-loop\n",
        "    heads_channels = {'human': 5313, 'mouse': 1643}\n",
        "    dropout_rate = 0.4\n",
        "    assert channels % num_heads == 0, ('channels needs to be divisible '\n",
        "                                       f'by {num_heads}')\n",
        "    whole_attention_kwargs = {\n",
        "        'attention_dropout_rate': 0.05,\n",
        "        'initializer': None,\n",
        "        'key_size': 64,\n",
        "        'num_heads': num_heads,\n",
        "        'num_relative_position_features': channels // num_heads,\n",
        "        'positional_dropout_rate': 0.01,\n",
        "        'relative_position_functions': [\n",
        "            'positional_features_exponential',\n",
        "            'positional_features_central_mask',\n",
        "            'positional_features_gamma'\n",
        "        ],\n",
        "        'relative_positions': True,\n",
        "        'scaling': True,\n",
        "        'value_size': channels // num_heads,\n",
        "        'zero_initialize': True\n",
        "    }\n",
        "\n",
        "    trunk_name_scope = tf.name_scope('trunk')\n",
        "    trunk_name_scope.__enter__()\n",
        "    # lambda is used in Sequential to construct the module under tf.name_scope.\n",
        "    def conv_block(filters, width=1, w_init=None, name='conv_block', **kwargs):\n",
        "      return Sequential(lambda: [\n",
        "          snt.distribute.CrossReplicaBatchNorm(\n",
        "              create_scale=True,\n",
        "              create_offset=True,\n",
        "              scale_init=snt.initializers.Ones(),\n",
        "              moving_mean=snt.ExponentialMovingAverage(0.9),\n",
        "              moving_variance=snt.ExponentialMovingAverage(0.9)),\n",
        "          gelu,\n",
        "          snt.Conv1D(filters, width, w_init=w_init, **kwargs)\n",
        "      ], name=name)\n",
        "\n",
        "    stem = Sequential(lambda: [\n",
        "        snt.Conv1D(channels // 2, 15),\n",
        "        Residual(conv_block(channels // 2, 1, name='pointwise_conv_block')),\n",
        "        pooling_module(pooling_type, pool_size=2),\n",
        "    ], name='stem')\n",
        "\n",
        "    filter_list = exponential_linspace_int(start=channels // 2, end=channels,\n",
        "                                           num=6, divisible_by=128)\n",
        "    conv_tower = Sequential(lambda: [\n",
        "        Sequential(lambda: [\n",
        "            conv_block(num_filters, 5),\n",
        "            Residual(conv_block(num_filters, 1, name='pointwise_conv_block')),\n",
        "            pooling_module(pooling_type, pool_size=2),\n",
        "            ],\n",
        "                   name=f'conv_tower_block_{i}')\n",
        "        for i, num_filters in enumerate(filter_list)], name='conv_tower')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko4GN0-xoWzm"
      },
      "source": [
        "Continuing from the previous part:\n",
        "\n",
        "- The code defines a helper function  `transformer_mlp() ` that constructs a multi-layer perceptron  `(MLP) ` block for the transformer. It consists of layer normalization, linear transformations, dropout, and ReLU activation.\n",
        "\n",
        "- The  `transformer ` module is defined as a sequential composition of `transformer` blocks. Each `transformer` block consists of a residual connection between a multi-head attention module and an  `MLP block `.\n",
        "\n",
        "- A  `crop_final ` module is defined to crop the final output sequence to a specific target length  `(TARGET_LENGTH) `.\n",
        "\n",
        "- The  `final_pointwise ` module is defined as a sequential composition of a pointwise convolution, dropout, and GELU activation.\n",
        "\n",
        "- The  `_trunk ` attribute is defined as a sequential composition of the previously defined modules  `(stem, conv_tower, transformer, crop_final, final_pointwise) `.\n",
        "\n",
        "- The ` _heads ` attribute is defined as a dictionary that maps different heads  `(e.g., 'human', 'mouse') ` to sequential modules consisting of a linear transformation and a softplus activation function.\n",
        "\n",
        "- The  `trunk ` property returns the ` _trunk ` attribute.\n",
        "\n",
        "- The  `heads ` property returns the  `_heads ` attribute.\n",
        "\n",
        "This completes the definition of the `Enformer` class. It provides the trunk and head modules of the model as properties and allows access to these components for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zux4GwjbogYI"
      },
      "outputs": [],
      "source": [
        "    # Transformer.\n",
        "    def transformer_mlp():\n",
        "      return Sequential(lambda: [\n",
        "          snt.LayerNorm(axis=-1, create_scale=True, create_offset=True),\n",
        "          snt.Linear(channels * 2),\n",
        "          snt.Dropout(dropout_rate),\n",
        "          tf.nn.relu,\n",
        "          snt.Linear(channels),\n",
        "          snt.Dropout(dropout_rate)], name='mlp')\n",
        "\n",
        "    transformer = Sequential(lambda: [\n",
        "        Sequential(lambda: [\n",
        "            Residual(Sequential(lambda: [\n",
        "                snt.LayerNorm(axis=-1,\n",
        "                              create_scale=True, create_offset=True,\n",
        "                              scale_init=snt.initializers.Ones()),\n",
        "                attention_module.MultiheadAttention(**whole_attention_kwargs,\n",
        "                                                    name=f'attention_{i}'),\n",
        "                snt.Dropout(dropout_rate)], name='mha')),\n",
        "            Residual(transformer_mlp())], name=f'transformer_block_{i}')\n",
        "        for i in range(num_transformer_layers)], name='transformer')\n",
        "\n",
        "    crop_final = TargetLengthCrop1D(TARGET_LENGTH, name='target_input')\n",
        "\n",
        "    final_pointwise = Sequential(lambda: [\n",
        "        conv_block(channels * 2, 1),\n",
        "        snt.Dropout(dropout_rate / 8),\n",
        "        gelu], name='final_pointwise')\n",
        "\n",
        "    self._trunk = Sequential([stem,\n",
        "                              conv_tower,\n",
        "                              transformer,\n",
        "                              crop_final,\n",
        "                              final_pointwise],\n",
        "                             name='trunk')\n",
        "    trunk_name_scope.exit(None, None, None)\n",
        "\n",
        "    with tf.name_scope('heads'):\n",
        "      self._heads = {\n",
        "          head: Sequential(\n",
        "              lambda: [snt.Linear(num_channels), tf.nn.softplus],\n",
        "              name=f'head_{head}')\n",
        "          for head, num_channels in heads_channels.items()\n",
        "      }\n",
        "    # pylint: enable=g-complex-comprehension,g-long-lambda,cell-var-from-loop\n",
        "\n",
        "  @property\n",
        "  def trunk(self):\n",
        "    return self._trunk\n",
        "\n",
        "  @property\n",
        "  def heads(self):\n",
        "    return self._heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAG5YkuFom46"
      },
      "source": [
        "The code use the following methods in the Enformer class:\n",
        "\n",
        "- The  `call() ` method is the main entry point for the model. It takes  inputs  `(inputs tensor) ` and a boolean flag indicating whether the model is in training mode  `(is_training) `. It first applies the  `trunk ` module to the inputs to obtain the trunk embedding. Then, it applies each  `head ` module to the trunk embedding and returns a dictionary where the keys are the head names and the values are the corresponding output tensors.\n",
        "\n",
        "- The  `predict_on_batch ` method is decorated with  `@tf.function() ` and  `input_signature ` to enable TensorFlow's graph mode and define the input signature for the method. It takes a  `batch of inputs (x) ` and calls the model's  `call() `  method with  `is_training=False `. This method is used for  `SavedModel `.\n",
        "\n",
        "These methods allow you to pass inputs to the Enformer model and obtain predictions from the model's heads. The  `predict_on_batch() ` method is specifically designed for use with  `SavedModel `, which is a serialization format for `TensorFlow` models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJg-8Ig-onvS"
      },
      "outputs": [],
      "source": [
        "def call(self, inputs: tf.Tensor,\n",
        "               is_training: bool) -> Dict[str, tf.Tensor]:\n",
        "    trunk_embedding = self.trunk(inputs, is_training=is_training)\n",
        "    return {\n",
        "        head: head_module(trunk_embedding, is_training=is_training)\n",
        "        for head, head_module in self.heads.items()\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, SEQUENCE_LENGTH, 4], tf.float32)])\n",
        "  def predict_on_batch(self, x):\n",
        "    \"\"\"Method for SavedModel.\"\"\"\n",
        "    return self(x, is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWuuH1yyorDI"
      },
      "source": [
        "The  `TargetLengthCrop1D ` class is a module in the model that is responsible for cropping the sequence length of the inputs to match a desired target length. Here's a breakdown of the class:\n",
        "\n",
        "- The  `init() ` method initializes the module. It takes the `target_length` parameter, which specifies the desired length for the cropped sequence. If `target_length` is None, no cropping is performed. The name parameter is used to assign a name to the module.\n",
        "\n",
        "- The  `call() ` method is the main entry point for the module. It takes inputs as the input tensor and performs the cropping operation. If the `target_length` is None, the inputs are returned as is. Otherwise, the method calculates the amount of trimming needed to match the target `length` and performs the `cropping` operation on the last dimension of the inputs `tensor`. The resulting `tensor` is then returned.\n",
        "\n",
        "The  `TargetLengthCrop1D ` module is used in the `Enformer` model to crop the sequence length of the inputs after the `transformer` module and before the final pointwise convolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRB_Q2QLouv5"
      },
      "outputs": [],
      "source": [
        "class TargetLengthCrop1D(snt.Module):\n",
        "  \"\"\"Crop sequence to match the desired target length.\"\"\"\n",
        "\n",
        "  def init(self,\n",
        "               target_length: Optional[int],\n",
        "               name: str = 'target_length_crop'):\n",
        "    super().init(name=name)\n",
        "    self._target_length = target_length\n",
        "\n",
        "  def call(self, inputs):\n",
        "    if self._target_length is None:\n",
        "      return inputs\n",
        "    trim = (inputs.shape[-2] - self._target_length) // 2\n",
        "    if trim < 0:\n",
        "      raise ValueError('inputs longer than target length')\n",
        "    elif trim == 0:\n",
        "      return inputs\n",
        "    else:\n",
        "      return inputs[..., trim:-trim, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVm-5yUWozog"
      },
      "source": [
        "The  `Sequential ` class is a custom module that allows for creating sequential modules where the  `is_training ` flag is automatically passed to the modules that accept it. Here's a breakdown of the class:\n",
        "\n",
        "- The  `init() ` method initializes the Sequential module. It takes the layers parameter, which can be a list of callable objects or a callable that returns an iterable of `snt.Module` instances. If layers is `None`, an empty list is assigned to `self._layers`. Otherwise, the callable objects are called (if needed) and added to `self._layers`.\n",
        "\n",
        "- The `call()` method is the main entry point for the module. It takes inputs as the input `tensor` and  `is_training ` as the boolean flag indicating whether the model is in training mode. It iterates over the layers in  `self._layers ` and applies each layer to the outputs tensor. If a layer accepts the  `is_training ` argument, it is passed along with the inputs tensor. Otherwise, only the inputs tensor is passed. The resulting tensor is then returned.\n",
        "\n",
        "The  `Sequential ` module provides a convenient way to create sequential modules while automatically handling the `is_training` flag. It is used in the Enformer model to define the stem, `conv_tower`, transformer, `crop_final`, and `final_pointwise` modules.\n",
        "\n",
        "The  `pooling_module ` function is a utility function that returns a pooling module based on the specified kind and  `pool_size `. If kind is  `'attention' `, it returns an instance of  `SoftmaxPooling1D ` with the specified  `pool_size ` and other parameters. If `kind` is  `'max' `, it returns an instance of  `tf.keras.layers.MaxPool1D ` with the specified  `pool_size ` and  `padding `. If `kind` is neither  `'attention' ` nor  `'max' `, a  `ValueError ` is raised. This function is used in the Enformer model to define the pooling modules for the  `conv_tower `."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzvTssl8o0kx"
      },
      "outputs": [],
      "source": [
        "class Sequential(snt.Module):\n",
        "  \"\"\"snt.Sequential automatically passing is_training where it exists.\"\"\"\n",
        "\n",
        "  def init(self,\n",
        "               layers: Optional[Union[Callable[[], Iterable[snt.Module]],\n",
        "                                      Iterable[Callable[..., Any]]]] = None,\n",
        "               name: Optional[Text] = None):\n",
        "    super().init(name=name)\n",
        "    if layers is None:\n",
        "      self._layers = []\n",
        "    else:\n",
        "      # layers wrapped in a lambda function to have a common namespace.\n",
        "      if hasattr(layers, 'call'):\n",
        "        layers = layers()\n",
        "      self._layers = [layer for layer in layers if layer is not None]\n",
        "\n",
        "  def call(self, inputs: tf.Tensor, is_training: bool, **kwargs):\n",
        "    outputs = inputs\n",
        "    for _, mod in enumerate(self._layers):\n",
        "      if accepts_is_training(mod):\n",
        "        outputs = mod(outputs, is_training=is_training, **kwargs)\n",
        "      else:\n",
        "        outputs = mod(outputs, **kwargs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def pooling_module(kind, pool_size):\n",
        "  \"\"\"Pooling module wrapper.\"\"\"\n",
        "  if kind == 'attention':\n",
        "    return SoftmaxPooling1D(pool_size=pool_size, per_channel=True,\n",
        "                            w_init_scale=2.0)\n",
        "  elif kind == 'max':\n",
        "    return tf.keras.layers.MaxPool1D(pool_size=pool_size, padding='same')\n",
        "  else:\n",
        "    raise ValueError(f'Invalid pooling kind: {kind}.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZBdGGrXo4cg"
      },
      "source": [
        "The  `SoftmaxPooling1D ` class is a custom pooling operation that performs pooling with optional weights. Here's a breakdown of the class:\n",
        "\n",
        "- The  `init() ` method initializes the  `SoftmaxPooling1D ` module. It takes several parameters:\n",
        "  -  `pool_size `: The pooling size, which is the same as in  `Max/AvgPooling `.\n",
        "  -  `per_channel `: A boolean flag indicating whether the  `logits/softmax ` weights should be computed for each channel separately `(True)` or whether the same weights should be used across all channels `(False)`.\n",
        "  -  `w_init_scale `: A float value used as a scaling factor for initializing the weights. When  `w_init_scale ` is  `0.0 `, it is equivalent to average pooling. When  `w_init_scale ` is around  `2.0 ` and  `per_channel ` is `False`, it is equivalent to max pooling.\n",
        "  -  `name `: The name of the module.\n",
        "\n",
        "- The  `initialize()` method is a private method that initializes the internal variables of the module. It takes  `num_features ` as an argument and initializes the `_logit_linear` variable with a `snt.Linear` module. The output size of the linear layer is set to `num_features` if  `per_channel ` is `True`, otherwise it is set to `1`. The linear layer is initialized with an identity initializer scaled by  `w_init_scale `. This method is called only once to initialize the module.\n",
        "\n",
        "- The  `call() ` method is the main entry point for the module. It takes inputs as the input tensor and performs the  `softmax ` pooling operation. It reshapes the input tensor into a shape that can be used for pooling. The reshaping is done to group adjacent elements within the pooling size. The logits for the  `softmax ` weights are computed using the  `_logit_linear ` module. The input tensor is multiplied `element-wise` with the  `softmax ` weights and then reduced along the pooling axis. The resulting `tensor` is returned as the output.\n",
        "\n",
        "The  `SoftmaxPooling1D ` module provides a flexible pooling operation with optional per-channel weights. It is used in the `Enformer` model as one of the options for pooling in the  `conv_tower ` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEdzqc2oo8hM"
      },
      "outputs": [],
      "source": [
        "class SoftmaxPooling1D(snt.Module):\n",
        "  \"\"\"Pooling operation with optional weights.\"\"\"\n",
        "\n",
        "  def init(self,\n",
        "               pool_size: int = 2,\n",
        "               per_channel: bool = False,\n",
        "               w_init_scale: float = 0.0,\n",
        "               name: str = 'softmax_pooling'):\n",
        "    \"\"\"Softmax pooling.\n",
        "\n",
        "    Args:\n",
        "      pool_size: Pooling size, same as in Max/AvgPooling.\n",
        "      per_channel: If True, the logits/softmax weights will be computed for\n",
        "        each channel separately. If False, same weights will be used across all\n",
        "        channels.\n",
        "      w_init_scale: When 0.0 is equivalent to avg pooling, and when\n",
        "        ~2.0 and per_channel=False it's equivalent to max pooling.\n",
        "      name: Module name.\n",
        "    \"\"\"\n",
        "    super().init(name=name)\n",
        "    self._pool_size = pool_size\n",
        "    self._per_channel = per_channel\n",
        "    self._w_init_scale = w_init_scale\n",
        "    self._logit_linear = None\n",
        "\n",
        "  @snt.once\n",
        "  def _initialize(self, num_features):\n",
        "    self._logit_linear = snt.Linear(\n",
        "        output_size=num_features if self._per_channel else 1,\n",
        "        with_bias=False,  # Softmax is agnostic to shifts.\n",
        "        w_init=snt.initializers.Identity(self._w_init_scale))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    _, length, num_features = inputs.shape\n",
        "    self._initialize(num_features)\n",
        "    inputs = tf.reshape(\n",
        "        inputs,\n",
        "        (-1, length // self._pool_size, self._pool_size, num_features))\n",
        "    return tf.reduce_sum(\n",
        "        inputs * tf.nn.softmax(self._logit_linear(inputs), axis=-2),\n",
        "        axis=-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWgpvIT3pGhS"
      },
      "source": [
        "Now the code provide some more module for residual and training model\n",
        "\n",
        "1.  `Residual ` class: This is a module representing a residual block. It takes a module as an argument, which is another Sonnet module, and applies residual connection by adding the input tensor to the output of the module.\n",
        "\n",
        "2.  `gelu() ` function: This function applies the  `Gaussian Error Linear Unit (GELU) ` activation function to the input tensor. It uses an approximation described in the original paper.\n",
        "\n",
        "3.  `one_hot_encode() ` function: This function performs one-hot encoding of a DNA sequence. It takes a DNA sequence as a string, along with optional parameters such as the alphabet, neutral alphabet, neutral value, and data type. It returns the one-hot encoded representation of the sequence as a NumPy array.\n",
        "\n",
        "4.  `exponential_linspace_int() ` function: This function generates a list of exponentially increasing values of integers. It takes the `start`, `end`, and the number of values to generate, along with an optional parameter to ensure the values are divisible by a specified number. The function returns a list of integers.\n",
        "\n",
        "5.  `accepts_is_training ` function: This function checks if a given module accepts an  `is_training ` argument in its  `call()` method. It uses inspect.signature to inspect the signature of the module's `call()` method and checks if the  `is_training ` parameter is present.\n",
        "\n",
        "These components provide additional functionality and utility functions used in the Enformer model to handle residual connections, activation functions, one-hot encoding, and other operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMIrCK0MpHmQ"
      },
      "outputs": [],
      "source": [
        "class Residual(snt.Module):\n",
        "  \"\"\"Residual block.\"\"\"\n",
        "\n",
        "  def init(self, module: snt.Module, name='residual'):\n",
        "    super().init(name=name)\n",
        "    self._module = module\n",
        "\n",
        "  def call(self, inputs: tf.Tensor, is_training: bool, *args,\n",
        "               **kwargs) -> tf.Tensor:\n",
        "    return inputs + self._module(inputs, is_training, *args, **kwargs)\n",
        "\n",
        "\n",
        "def gelu(x: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Applies the Gaussian error linear unit (GELU) activation function.\n",
        "\n",
        "  Using approximiation in section 2 of the original paper:\n",
        "  https://arxiv.org/abs/1606.08415\n",
        "\n",
        "  Args:\n",
        "    x: Input tensor to apply gelu activation.\n",
        "  Returns:\n",
        "    Tensor with gelu activation applied to it.\n",
        "  \"\"\"\n",
        "  return tf.nn.sigmoid(1.702 * x) * x\n",
        "\n",
        "\n",
        "def one_hot_encode(sequence: str,\n",
        "                   alphabet: str = 'ACGT',\n",
        "                   neutral_alphabet: str = 'N',\n",
        "                   neutral_value: Any = 0,\n",
        "                   dtype=np.float32) -> np.ndarray:\n",
        "  \"\"\"One-hot encode sequence.\"\"\"\n",
        "  def to_uint8(string):\n",
        "    return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n",
        "  hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n",
        "  hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
        "  hash_table[to_uint8(neutral_alphabet)] = neutral_value\n",
        "  hash_table = hash_table.astype(dtype)\n",
        "  return hash_table[to_uint8(sequence)]\n",
        "\n",
        "\n",
        "def exponential_linspace_int(start, end, num, divisible_by=1):\n",
        "  \"\"\"Exponentially increasing values of integers.\"\"\"\n",
        "  def _round(x):\n",
        "    return int(np.round(x / divisible_by) * divisible_by)\n",
        "\n",
        "  base = np.exp(np.log(end / start) / (num - 1))\n",
        "  return [_round(start * base**i) for i in range(num)]\n",
        "\n",
        "\n",
        "def accepts_is_training(module):\n",
        "  return 'is_training' in list(inspect.signature(module.call).parameters)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}