{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial: Training Enformer - A Step-by-Step Guide**\n",
        "\n",
        "Welcome to our tutorial page on training Enformers! Here, we will provide you with a detailed understanding of how Enformers work. You will be able to explore the code implementation for each part and observe the training, testing, and evaluation results of this method.\n",
        "\n",
        "Before diving into the tutorial, let's go through the necessary steps to train the data:\n",
        "\n",
        "**Steps:**\n",
        "1. Set up the `tf.data.Dataset` by directly accessing the Basenji2 data on GCS at `gs://basenji_barnyard/data`.\n",
        "2. Begin training the model by performing a few steps, alternating between training on human and mouse data batches.\n",
        "3. Evaluate the model's performance on human and mouse genomes.\n",
        "\n",
        "To gain more insights into the data and understand how it is sent to this method, we recommend reading the report prior to accessing this page.\n",
        "\n",
        "We hope this tutorial provides you with a comprehensive understanding of Enformer training. If you have any questions or require further assistance, please don't hesitate to reach out."
      ],
      "metadata": {
        "id": "Od3M_x6Z6fwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial line of code utilizes the pip package manager to install two Python packages, namely \"dm-sonnet\" and \"tqdm\".\n",
        "\n",
        "1- \"dm-sonnet\" serves as a deep learning library that is constructed on TensorFlow. Its purpose is to simplify the process of constructing and training neural networks by providing a collection of abstractions and modules.\n",
        "\n",
        "2- On the other hand, \"tqdm\" is a Python library that facilitates the creation of progress bars and the visualization of progress for iterations or tasks within the command line interface. The presence of an exclamation mark at the start of the line indicates that the code is being executed within a Jupyter Notebook or an IPython environment. In such environments, the exclamation mark is employed to directly execute shell commands from the notebook. In this particular case, it is utilized to execute the \"pip install\" command for the installation of the necessary packages."
      ],
      "metadata": {
        "id": "EqbVjem3daVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE66P4k-WF7m",
        "outputId": "dc2e154c-519e-43f2-aa93-464ce8bfa190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (0.1.8)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.22.4)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (0.8.10)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.14.1)\n",
            "Installing collected packages: dm-sonnet\n",
            "Successfully installed dm-sonnet-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dm-sonnet tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wget command is utilized to download two Python source code files from specific URLs:\n",
        "\n",
        "1- The first wget command downloads a source code file called attention_module.py from the GitHub repository: https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/attention_module.py.\n",
        "2- The second wget command downloads a source code file named enformer.py from the GitHub repository: https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/enformer.py.\n",
        "\n",
        "To run the command quietly without displaying any output or progress information, the -q option is used with wget. This approach is commonly adopted in scripts or automation tasks to maintain a clean terminal or notebook output without unnecessary clutter.\n",
        "\n",
        "By executing these wget commands, the code fetches the Enformer model or module's source code files from the specified URLs. These downloaded files can be subsequently employed locally for further development or incorporated into Python code as needed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RQ2g59h_d8aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get enformer source code\n",
        "!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/attention_module.py\n",
        "!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/enformer.py"
      ],
      "metadata": {
        "id": "kxx1cf42eFrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that attention_module.py and enformer.py is explained in different .ipynb files in this GitHub page."
      ],
      "metadata": {
        "id": "5fS5flvBeLcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries**"
      ],
      "metadata": {
        "id": "6XjKJ5RVeTm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code begins by importing the TensorFlow library using the line import tensorflow as tf.\n",
        "\n",
        "To ensure that a GPU is enabled, it includes an assertion statement. The code verifies the presence of any physical GPU devices by invoking tf.config.list_physical_devices('GPU'). If there are no available GPU devices, the code raises an AssertionError with the message \"Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU\". This assertion guarantees that the code can leverage the GPU for accelerated computations.\n",
        "\n",
        "Next, the code sets an environment variable named TF_ENABLE_GPU_GARBAGE_COLLECTION to false. This variable impacts the garbage collection behavior within TensorFlow when utilizing a GPU. By assigning it a value of false, the code disables GPU-specific garbage collection optimizations. This can be beneficial for simplifying the debugging process, especially when encountering out-of-memory (OOM) errors during GPU computations.\n",
        "\n",
        "In summary, this code snippet ensures the availability of a GPU and adjusts certain TensorFlow settings pertaining to GPU usage and memory management. These adjustments are made for the purpose of facilitating debugging activities."
      ],
      "metadata": {
        "id": "yDCVtD7DeZpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Make sure the GPU is enabled \n",
        "#assert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU'\n",
        "\n",
        "# Easier debugging of OOM\n",
        "%env TF_ENABLE_GPU_GARBAGE_COLLECTION=fals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhiDafCgeSDj",
        "outputId": "a0ba551e-4f16-4791-be66-be34ccd7cc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TF_ENABLE_GPU_GARBAGE_COLLECTION=fals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports several Python libraries and modules:\n",
        "\n",
        "1. sonnet from the snt module: Sonnet is a deep learning library built on top of TensorFlow, and snt is a sub-module within Sonnet. It provides additional functionality and abstractions for building neural networks.\n",
        "\n",
        "2. tqdm: This library is used for creating progress bars and visualizing the progress of iterations or tasks in the command line interface.\n",
        "\n",
        "3. IPython.display from the clear_output module: This module provides functionality for controlling the display in IPython environments. clear_output is a function that clears the output of the cell or console.\n",
        "\n",
        "4. numpy as np: NumPy is a fundamental library for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently.\n",
        "\n",
        "5. pandas as pd: Pandas is a powerful library for data manipulation and analysis. It provides data structures and functions to efficiently work with structured data, such as tables or CSV files.\n",
        "\n",
        "6. time: This module provides functions for working with time-related operations, such as measuring elapsed time or introducing delays in the code.\n",
        "\n",
        "7. os: The os module provides a way to use operating system-dependent functionality, such as interacting with the file system, working with environment variables, and executing system commands.\n",
        "\n",
        "By importing these libraries and modules, the code gains access to their respective functionalities, allowing for easier and more efficient development of the subsequent code.\n"
      ],
      "metadata": {
        "id": "EgU-qzr7ekpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sonnet as snt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "59V6pHblepf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses an assert statement to check the version number of the sonnet library (snt). Specifically, it verifies that the version number starts with '2.0'.\n",
        "\n",
        "The assert statement checks if the condition provided is True. If the condition is False, it raises an AssertionError with an optional error message.\n",
        "\n",
        "In this case, the code asserts that the version number of sonnet starts with '2.0'. If the version number does not meet this condition, an AssertionError will be raised. This assertion is typically used to ensure compatibility or specific features in the code that rely on a particular version of the sonnet library."
      ],
      "metadata": {
        "id": "VU3FcC3DfFdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assert snt.version.startswith('2.0')"
      ],
      "metadata": {
        "id": "9pwB_KnEak3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code !nvidia-smi is used in a Colab notebook or Jupyter notebook to display information about the GPU(s) available in the environment. It executes the shell command nvidia-smi, which is a tool provided by NVIDIA to monitor and manage NVIDIA GPU devices."
      ],
      "metadata": {
        "id": "tib8_gMxfNF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU colab has T4 with 16 GiB of memory\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGQs_rYVfTwr",
        "outputId": "03e0f804-a008-4d92-fe7b-91e4efad9b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code**"
      ],
      "metadata": {
        "id": "GuE7_88BfWM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing enformer library which is class defined by the author. The class is fully explained in another file in this GitHub page. \n"
      ],
      "metadata": {
        "id": "W9zjExfBfgvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enformer"
      ],
      "metadata": {
        "id": "fe8miausudTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines the targets_txt variable, which is a formatted string holding the URL to the target information text file specific to the given organism. The URL is constructed using an f-string, where the value of the organism is inserted into the URL.\n",
        "\n",
        "To read the contents of the target information text file into a DataFrame, the function employs pd.read_csv() from the pandas library. It assumes that the text file is tab-separated, specified by the sep='\\t' parameter.\n",
        "\n",
        "The resulting DataFrame, containing the target information, is then returned as the output of the function."
      ],
      "metadata": {
        "id": "1WvwpAL9fox8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title get_targets(organism)\n",
        "def get_targets(organism):\n",
        "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
        "  return pd.read_csv(targets_txt, sep='\\t')"
      ],
      "metadata": {
        "id": "bIcqy0zkfcAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the author presents a function called get_dataset, which serves the purpose of retrieving a dataset for a designated organism and subset. The dataset is acquired from TFRecord files and processed using TensorFlow.\n",
        "\n",
        "Here is the explanation of the code:\n",
        "\n",
        "1. The code begins by defining the organism_path function. This function generates the path to the directory containing the data pertaining to a specific organism.\n",
        "\n",
        "2. The get_dataset function is created, which accepts three arguments: organism (specifying the desired organism), subset (indicating the specific subset of the dataset, such as 'train', 'valid', or 'test'), and num_threads (representing the number of parallel threads utilized for reading the TFRecord files).\n",
        "\n",
        "3. Within the get_dataset function, a call is made to get_metadata, which retrieves metadata associated with the organism's data. This information may include the count of targets, sequence lengths, and other relevant details.\n",
        "\n",
        "4. The tfrecord_files function is implemented to generate a list of TFRecord file paths corresponding to the specified organism and subset.\n",
        "\n",
        "5. A TFRecordDataset object is created and assigned to the dataset variable. This object facilitates the reading of the TFRecord files, utilizing parallel reads and zlib compression.\n",
        "\n",
        "6. The dataset is mapped using the deserialize function. This function performs the deserialization process on the byte-encoded examples found within the TFRecord files, converting them into TensorFlow tensors.\n",
        "\n",
        "7. Finally, the processed dataset is returned as the output of the get_dataset function.\n",
        "\n",
        "In addition to the get_dataset function, several auxiliary functions (get_metadata, tfrecord_files, and deserialize) are defined to assist in obtaining metadata, generating file paths, and deserializing TFRecord examples, respectively.\n",
        "\n",
        "Overall, this code snippet enables the retrieval of a dataset for a particular organism and subset from TFRecord files. The dataset can then be further processed and analyzed using TensorFlow's capabilities."
      ],
      "metadata": {
        "id": "5oFrRifjfw47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title get_dataset(organism, subset, num_threads=8)\n",
        "import glob\n",
        "import json\n",
        "import functools\n",
        "\n",
        "\n",
        "def organism_path(organism):\n",
        "  return os.path.join('gs://basenji_barnyard/data', organism)\n",
        "\n",
        "\n",
        "def get_dataset(organism, subset, num_threads=8):\n",
        "  metadata = get_metadata(organism)\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
        "                                    compression_type='ZLIB',\n",
        "                                    num_parallel_reads=num_threads)\n",
        "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
        "                        num_parallel_calls=num_threads)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def get_metadata(organism):\n",
        "  # Keys:\n",
        "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
        "  # pool_width, crop_bp, target_length\n",
        "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
        "  with tf.io.gfile.GFile(path, 'r') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "\n",
        "def tfrecord_files(organism, subset):\n",
        "  # Sort the values by int(*).\n",
        "  return sorted(tf.io.gfile.glob(os.path.join(\n",
        "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
        "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
        "\n",
        "\n",
        "def deserialize(serialized_example, metadata):\n",
        "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
        "  feature_map = {\n",
        "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
        "      'target': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  example = tf.io.parse_example(serialized_example, feature_map)\n",
        "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
        "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
        "  sequence = tf.cast(sequence, tf.float32)\n",
        "\n",
        "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
        "  target = tf.reshape(target,\n",
        "                      (metadata['target_length'], metadata['num_targets']))\n",
        "  target = tf.cast(target, tf.float32)\n",
        "\n",
        "  return {'sequence': sequence,\n",
        "          'target': target}"
      ],
      "metadata": {
        "id": "lAmGX4eZf4iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load the data set\n",
        "The code snippet is calling the get_targets function to retrieve the target information for the organism 'human'. It assigns the returned DataFrame to the variable df_targets_human and then displays the first few rows using the head() method.\n",
        "\n",
        "Assuming the get_targets function is defined properly, it should retrieve the target information for the 'human' organism from a specific URL. The returned DataFrame, df_targets_human, contains the target data, and calling head() on it displays the first few rows of the DataFrame."
      ],
      "metadata": {
        "id": "wd0cNGMXgNBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_targets_human = get_targets('human')\n",
        "df_targets_human.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JlTR5A2pgWLk",
        "outputId": "39996c90-4bc6-40ea-9e77-df00294b1f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index  genome   identifier  \\\n",
              "0      0       0  ENCFF833POA   \n",
              "1      1       0  ENCFF110QGM   \n",
              "2      2       0  ENCFF880MKD   \n",
              "3      3       0  ENCFF463ZLQ   \n",
              "4      4       0  ENCFF890OGQ   \n",
              "\n",
              "                                                file  clip  scale sum_stat  \\\n",
              "0  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "1  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "2  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "3  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "4  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "\n",
              "                                         description  \n",
              "0  DNASE:cerebellum male adult (27 years) and mal...  \n",
              "1  DNASE:frontal cortex male adult (27 years) and...  \n",
              "2                                      DNASE:chorion  \n",
              "3  DNASE:Ishikawa treated with 0.02% dimethyl sul...  \n",
              "4                                      DNASE:GM03348  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>genome</th>\n",
              "      <th>identifier</th>\n",
              "      <th>file</th>\n",
              "      <th>clip</th>\n",
              "      <th>scale</th>\n",
              "      <th>sum_stat</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF833POA</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:cerebellum male adult (27 years) and mal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF110QGM</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:frontal cortex male adult (27 years) and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF880MKD</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:chorion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF463ZLQ</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:Ishikawa treated with 0.02% dimethyl sul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF890OGQ</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:GM03348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the author use code to generates three datasets: human_dataset, mouse_dataset, and human_mouse_dataset. Each dataset is acquired by utilizing the get_dataset function and specifying the organism ('human' or 'mouse') and subset ('train').\n",
        "\n",
        "Here is the explanation of the code:\n",
        "\n",
        "1. The human_dataset is obtained by invoking get_dataset with the arguments 'human' and 'train'. This call retrieves the dataset for the 'human' organism and the 'train' subset.\n",
        "\n",
        "2. Similarly, the mouse_dataset is obtained by invoking get_dataset with the arguments 'mouse' and 'train'. This call retrieves the dataset for the 'mouse' organism and the 'train' subset.\n",
        "\n",
        "3. Both human_dataset and mouse_dataset are modified by applying the .batch(1) method, which groups the elements of the dataset into individual batches, each containing a single element. This approach ensures that each element is processed individually.\n",
        "\n",
        "4. The .repeat() method is additionally applied to both datasets. This method repeats the dataset indefinitely, allowing for multiple iterations during training or evaluation.\n",
        "\n",
        "5. The human_dataset and mouse_dataset are merged using the tf.data.Dataset.zip() function, creating a new dataset named human_mouse_dataset. This resulting dataset contains pairs of samples, with each sample originating from either the 'human' or 'mouse' dataset.\n",
        "\n",
        "6. Lastly, the .prefetch(2) method is invoked on human_mouse_dataset. This operation prefetches and buffers up to 2 elements, enhancing training performance by overlapping data preprocessing and model training.\n",
        "\n",
        "In summary, this code generates datasets for the 'human' and 'mouse' organisms, and then combines them into a single dataset (human_mouse_dataset) containing pairs of samples. The resulting dataset is suitable for further processing or training purposes."
      ],
      "metadata": {
        "id": "1PjcefQZghtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_dataset = get_dataset('human', 'train').batch(1).repeat()\n",
        "mouse_dataset = get_dataset('mouse', 'train').batch(1).repeat()\n",
        "human_mouse_dataset = tf.data.Dataset.zip((human_dataset, mouse_dataset)).prefetch(2)"
      ],
      "metadata": {
        "id": "zG1mCe5SDmHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet creates an iterator for the mouse_dataset using the iter() function and assigns it to the variable it. It then retrieves the next element from the iterator using the next() function and assigns it to the variable example.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The iter() function is called with the mouse_dataset as an argument to create an iterator object. An iterator allows iterating over the elements of a dataset.\n",
        "- The next() function is used to retrieve the next element from the iterator it. Each time next() is called, it returns the next element of the dataset.\n",
        "- The retrieved element is assigned to the variable example.\n",
        "\n",
        "After executing this code, the example variable will contain the next element from the mouse_dataset. You can then use this example to access and manipulate the data within that element for further processing or analysis."
      ],
      "metadata": {
        "id": "sdQRwok1gqvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(mouse_dataset)\n",
        "example = next(it)"
      ],
      "metadata": {
        "id": "EhdlJ75EDsKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step demonstrates how to iterate over the human_mouse_dataset and print information about the elements in each iteration.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The iter() function is called with the human_mouse_dataset as an argument to create an iterator object, which is assigned to the variable it.\n",
        "- The next() function is used to retrieve the next element from the iterator it. Each time the loop iterates, it retrieves the next element of the dataset.\n",
        "- Within the loop, a for loop is used to iterate over the range of the length of example, which is the number of elements in the current iteration of the dataset.\n",
        "- Inside the for loop, the organism ('human' or 'mouse') is printed based on the index i. It uses a list comprehension (['human', 'mouse'][i]) to choose the corresponding organism string.\n",
        "- The example[i] represents the current element of the dataset for the organism at index i. It is a dictionary containing different keys and their corresponding values.\n",
        "- The dictionary items are printed using a dictionary comprehension {k: (v.shape, v.dtype) for k, v in example[i].items()}. This displays the shape and data type of each value in the current element.\n",
        "\n",
        "By running this code, it will iterate over the human_mouse_dataset, print the organism label ('human' or 'mouse') for each element, and display the shape and data type information for each value within that element. This can be helpful for understanding the structure and characteristics of the dataset elements."
      ],
      "metadata": {
        "id": "s-__0DsTgxhK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeztqJZ74ixT",
        "outputId": "39dc4051-5a19-4443-b6b0-bf6869faf5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "human\n",
            "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 5313]), tf.float32)}\n",
            "mouse\n",
            "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 1643]), tf.float32)}\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "it = iter(human_mouse_dataset)\n",
        "example = next(it)\n",
        "for i in range(len(example)):\n",
        "  print(['human', 'mouse'][i])\n",
        "  print({k: (v.shape, v.dtype) for k,v in example[i].items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ],
      "metadata": {
        "id": "Pvu-Ncc-g3QR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step the author introduces a function called create_step_function, which is responsible for producing and returning a training step function tailored to a given model and optimizer.\n",
        "\n",
        "Here's the explanation of the code:\n",
        "\n",
        "The create_step_function function accepts two arguments: model (representing the model utilized for training) and optimizer (representing the optimizer employed to update the model's trainable variables).\n",
        "\n",
        "Within the function, a nested function named train_step is defined. This function is decorated with @tf.function, enabling it to be compiled and optimized using TensorFlow's AutoGraph functionality, leading to improved performance.\n",
        "\n",
        "The train_step function encompasses several parameters, including batch (representing the input batch comprising sequences and targets), head (signifying a specific output head of the model), and optimizer_clip_norm_global (an optional parameter for gradient clipping).\n",
        "\n",
        "Inside the train_step function, a gradient tape (tf.GradientTape) is employed to record the operations for automatic differentiation. The model's forward pass is executed by invoking model with the input sequences (batch['sequence']) while setting is_training to True. The desired output head (head) is extracted from the model's outputs.\n",
        "\n",
        "The loss is determined by comparing the predicted outputs (outputs) with the target values (batch['target']) using tf.keras.losses.poisson. The reduce_mean function is utilized to calculate the average loss across the batch.\n",
        "\n",
        "To compute the gradients of the loss with respect to the model's trainable variables, the gradient tape (tape.gradient) is leveraged.\n",
        "\n",
        "The computed gradients are subsequently applied to the model's trainable variables by invoking the optimizer's apply method, thereby updating the model's parameters.\n",
        "\n",
        "The loss is then returned as the output of the train_step function.\n",
        "\n",
        "Finally, the create_step_function function returns the train_step function. This train_step function can be utilized to execute a single training step on the model using the specified optimizer. It offers a convenient and efficient approach to train the model by encapsulating the necessary operations within a TensorFlow function."
      ],
      "metadata": {
        "id": "UxWbCZNEg8kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_step_function(model, optimizer):\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(batch, head, optimizer_clip_norm_global=0.2):\n",
        "    with tf.GradientTape() as tape:\n",
        "      outputs = model(batch['sequence'], is_training=True)[head]\n",
        "      loss = tf.reduce_mean(\n",
        "          tf.keras.losses.poisson(batch['target'], outputs))\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply(gradients, model.trainable_variables)\n",
        "\n",
        "    return loss\n",
        "  return train_step"
      ],
      "metadata": {
        "id": "jgPWA-GahAx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet sets up the learning rate, optimizer, model, and training step function for training a model using the Enformer architecture.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- A learning_rate variable is created using tf.Variable and initialized to 0. It is set as non-trainable by setting trainable=False. This variable is used to control the learning rate during training.\n",
        "- An Adam optimizer is created using snt.optimizers.Adam, and the learning_rate is set to the previously created variable learning_rate.\n",
        "- The num_warmup_steps variable is set to 5000, which represents the number of warm-up steps for the learning rate schedule.\n",
        "- The target_learning_rate is set to 0.0005, which represents the desired learning rate after the warm-up period.\n",
        "- An instance of the Enformer model is created with specific configurations, such as the number of channels, number of heads, number of transformer layers, and pooling type.\n",
        "- The create_step_function function is called with the model and optimizer as arguments, and the returned train_step function is assigned to the train_step variable.\n",
        "\n",
        "After executing this code, you can use the train_step function to perform a single training step on the model using the specified optimizer. The learning rate can be controlled by adjusting the value of the learning_rate variable. This setup provides the necessary components for training the Enformer model with the specified configurations."
      ],
      "metadata": {
        "id": "2qOJA8kGhImO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = tf.Variable(0., trainable=False, name='learning_rate')\n",
        "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
        "num_warmup_steps = 5000\n",
        "target_learning_rate = 0.0005\n",
        "\n",
        "model = enformer.Enformer(channels=1536 // 4,  # Use 4x fewer channels to train faster.\n",
        "                          num_heads=8,\n",
        "                          num_transformer_layers=11,\n",
        "                          pooling_type='max')\n",
        "\n",
        "train_step = create_step_function(model, optimizer)"
      ],
      "metadata": {
        "id": "FXOa1rtBhM9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, the code trains the model based on the specified configuration and outputs the loss and learning rate at the conclusion of each epoch. You can see the loss for human and mouse in this part, that would woth also if we consider learning rate also in our calculation and regards its effects on our results.\n",
        "\n",
        "Here's the explanation of the code:\n",
        "\n",
        "1. The steps_per_epoch variable is initialized to 20, indicating the number of steps to iterate through the dataset in each epoch.\n",
        "\n",
        "2. The num_epochs variable is set to 5, representing the total number of epochs during which the model will be trained.\n",
        "\n",
        "3. The data_it variable is created by invoking iter(human_mouse_dataset), generating an iterator for the human_mouse_dataset.\n",
        "\n",
        "4. A global step counter, global_step, is initialized to 0. This variable keeps track of the overall number of training steps across all epochs.\n",
        "\n",
        "5. The training process is executed using nested loops. The outer loop iterates over the range of num_epochs, while the inner loop iterates over the range of steps_per_epoch.\n",
        "\n",
        "6. Within the inner loop, the global_step is incremented by 1 to monitor the progress.\n",
        "\n",
        "7. Following the initial training step (global_step > 1), the learning rate is adjusted based on the current global step and the number of warm-up steps. The learning rate fraction is computed by dividing the current global step by the maximum of 1 and the number of warm-up steps. The learning rate is subsequently updated by assigning the target learning rate multiplied by the learning rate fraction to the learning_rate variable.\n",
        "\n",
        "8. The next(data_it) function is employed to retrieve the subsequent batch of data from the iterator. This function returns a tuple of batches, with each batch corresponding to the 'human' and 'mouse' organisms, respectively.\n",
        "\n",
        "9. The train_step function is invoked twice to execute the training step on both the 'human' and 'mouse' batches individually. The losses (loss_human and loss_mouse) are obtained.\n",
        "\n",
        "10. At the conclusion of each epoch, the loss values for both organisms and the current learning rate are printed.\n",
        "\n",
        "By executing this code, the model undergoes training for the specified number of epochs, and the loss and learning rate are displayed at the conclusion of each epoch. The training process entails iterating over the dataset, updating the model parameters, and adjusting the learning rate based on the global step and warm-up schedule."
      ],
      "metadata": {
        "id": "qsKKC6chhRl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrbDaOMWcFUl",
        "outputId": "6a42f69c-3003-47f2-a8d2-1b94c52eb57e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:24<00:00,  1.25s/it]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.774059 loss_mouse 0.94303024 learning_rate 2.0000002e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.0067647 loss_mouse 0.8752468 learning_rate 4.0000004e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.0471998 loss_mouse 0.89318746 learning_rate 6e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.010262 loss_mouse 1.02991 learning_rate 8.000001e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.111991 loss_mouse 0.84773445 learning_rate 1.0000001e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "steps_per_epoch = 20\n",
        "num_epochs = 5\n",
        "\n",
        "data_it = iter(human_mouse_dataset)\n",
        "global_step = 0\n",
        "for epoch_i in range(num_epochs):\n",
        "  for i in tqdm(range(steps_per_epoch)):\n",
        "    global_step += 1\n",
        "\n",
        "    if global_step > 1:\n",
        "      learning_rate_frac = tf.math.minimum(\n",
        "          1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))      \n",
        "      learning_rate.assign(target_learning_rate * learning_rate_frac)\n",
        "\n",
        "    batch_human, batch_mouse = next(data_it)\n",
        "\n",
        "    loss_human = train_step(batch=batch_human, head='human')\n",
        "    loss_mouse = train_step(batch=batch_mouse, head='mouse')\n",
        "\n",
        "  # End of epoch.\n",
        "  print('')\n",
        "  print('loss_human', loss_human.numpy(),\n",
        "        'loss_mouse', loss_mouse.numpy(),\n",
        "        'learning_rate', optimizer.learning_rate.numpy()\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**\n",
        "###pearsonR"
      ],
      "metadata": {
        "id": "c3ME8bBihdC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, the code snippet defines a function evaluate_model that evaluates the performance of a model on a given dataset and a specific output head.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The evaluate_model function takes three arguments: model (the model to be evaluated), dataset (the dataset on which evaluation will be performed), and head (the specific output head of the model).\n",
        "- Inside the function, a MetricDict object is created with the initial metric of 'PearsonR'. This object is used to store and compute evaluation metrics.\n",
        "- A nested function predict is defined using the @tf.function decorator. This function takes an input x and returns the predicted outputs of the model for the specified head. It sets is_training to False to ensure evaluation mode.\n",
        "- A loop is performed over the dataset using enumerate(dataset). The loop iterates over the batches of the dataset, and the index is stored in i and the batch data is stored in batch.\n",
        "- If a max_steps value is provided and the current iteration exceeds that value, the loop breaks.\n",
        "- Inside the loop, the metric.update_state method is called to update the metric with the ground truth targets (batch['target']) and the predicted outputs obtained by calling the predict function on the input sequences (batch['sequence']).\n",
        "- After iterating over the dataset, the evaluation result is obtained by calling metric.result(), which returns the computed evaluation metric.\n",
        "\n",
        "The evaluate_model function allows for evaluating the performance of a model on a given dataset and specific output head. It computes the specified evaluation metric by comparing the model's predictions with the ground truth targets."
      ],
      "metadata": {
        "id": "YLdn4vUrhpf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset, head, max_steps=None):\n",
        "  metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
        "  @tf.function\n",
        "  def predict(x):\n",
        "    return model(x, is_training=False)[head]\n",
        "\n",
        "  for i, batch in tqdm(enumerate(dataset)):\n",
        "    if max_steps is not None and i > max_steps:\n",
        "      break\n",
        "    metric.update_state(batch['target'], predict(batch['sequence']))\n",
        "\n",
        "  return metric.result()"
      ],
      "metadata": {
        "id": "WJVHFAzehtCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the code evaluates the model's performance on the 'human' dataset using the 'human' output head and computes the mean value of the evaluation metrics.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The evaluate_model function is called with the following arguments:\n",
        "  - model: The model to be evaluated.\n",
        "  - dataset: The dataset for evaluation, obtained by calling get_dataset('human', 'valid').batch(1).prefetch(2). It batches the data with a batch size of 1 and prefetches 2 batches for improved performance.\n",
        "  - head: The specific output head of the model to evaluate, which is set to 'human'.\n",
        "  - max_steps: The maximum number of steps to perform evaluation, which is set to 100.\n",
        "- The evaluation metrics for the 'human' dataset are stored in the metrics_human variable.\n",
        "- The computed mean values of the evaluation metrics are printed using a dictionary comprehension, where the key is the metric name and the value is the mean value obtained by calling .numpy().mean() on each metric.\n",
        "\n",
        "By running this code, the model will be evaluated on the 'human' dataset using the specified output head, and the mean values of the evaluation metrics will be displayed."
      ],
      "metadata": {
        "id": "WrZs6ziHhwGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_human = evaluate_model(model,\n",
        "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
        "                               head='human',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_human.items()})"
      ],
      "metadata": {
        "id": "BMylwJteEjE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also the code snippet evaluates the model's performance on the 'mouse' dataset using the 'mouse' output head and computes the mean value of the evaluation metrics.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The evaluate_model function is called with the following arguments:\n",
        "  - model: The model to be evaluated.\n",
        "  - dataset: The dataset for evaluation, obtained by calling get_dataset('mouse', 'valid').batch(1).prefetch(2). It batches the data with a batch size of 1 and prefetches 2 batches for improved performance.\n",
        "  - head: The specific output head of the model to evaluate, which is set to 'mouse'.\n",
        "  - max_steps: The maximum number of steps to perform evaluation, which is set to 100.\n",
        "- The evaluation metrics for the 'mouse' dataset are stored in the metrics_mouse variable.\n",
        "- The computed mean values of the evaluation metrics are printed using a dictionary comprehension, where the key is the metric name and the value is the mean value obtained by calling .numpy().mean() on each metric.\n",
        "\n",
        "By running this code, the model will be evaluated on the 'mouse' dataset using the specified output head, and the mean values of the evaluation metrics will be displayed."
      ],
      "metadata": {
        "id": "as0vB4XbiDsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY_wj95xiDtE",
        "outputId": "fea839f7-b6c9-46ed-aece-c56b02e9ea16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:21,  6.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{'PearsonR': 0.005183698}\n"
          ]
        }
      ],
      "source": [
        "metrics_mouse = evaluate_model(model,\n",
        "                               dataset=get_dataset('mouse', 'valid').batch(1).prefetch(2),\n",
        "                               head='mouse',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_mouse.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, lets see the results also on human"
      ],
      "metadata": {
        "id": "ySIZrf6i-Qc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57fNitK9hzwd",
        "outputId": "947aaadb-dad2-4a00-ddac-d765f65d782f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:23,  6.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{'PearsonR': 0.0028573992}\n"
          ]
        }
      ],
      "source": [
        "metrics_human = evaluate_model(model,\n",
        "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
        "                               head='human',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_human.items()})"
      ]
    }
  ]
}