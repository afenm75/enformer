{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od3M_x6Z6fwr"
      },
      "source": [
        "**Tutorial: Training Enformer - A Step-by-Step Guide**\n",
        "\n",
        "Welcome to our tutorial page on training Enformers! Here, we will provide you with a detailed understanding of how `Enformer` method works. You will be able to explore the code implementation for each part and observe the training, testing, and evaluation results of this method.\n",
        "\n",
        "Before diving into the tutorial, let's go through the necessary steps to train the data:\n",
        "\n",
        "**Steps:**\n",
        "1. Set up the `tf.data.Dataset` by accessing the Basenji2 data on Google Cloud Storage (GCS) at `gs://basenji_barnyard/data`. GCS is a cloud-based storage service. You can download the data by referring to the documentation or following the instructions provided in a resource like `gsutil`.\n",
        "\n",
        "2. Begin training the model by alternating between training on human and mouse data batches.\n",
        "\n",
        "3. Evaluate the model's performance on human and mouse genomes.\n",
        "\n",
        "The Enformer model utilizes a state-of-the-art architecture to predict genomic tracks from one-hot-encoded DNA sequences. This architecture consists of three main components: convolutional blocks with pooling, transformer blocks, and cropping layers. These components are followed by final pointwise convolutions that branch into two organism-specific network heads.\n",
        "\n",
        "The input to the Enformer model is a DNA sequence of length 196,608 bp, which is one-hot-encoded and used for prediction. For the human genome, the model produces 5,313 genomic tracks, while for the mouse genome, it generates 1,643 tracks. Each of these tracks has a length of 896, corresponding to 114,688 bp, which are aggregated into 128-bp bins.\n",
        "\n",
        "The convolutional blocks with pooling play a crucial role in reducing the spatial dimension from 196,608 bp to 1,536. This reduction ensures that each sequence position vector represents a 128 bp segment, allowing for efficient processing and analysis within the Enformer model.\n",
        "To gain more insights into the data and understand how it is sent to this method, we recommend reading the report prior to accessing this page.\n",
        " .\n",
        "\n",
        "We hope this tutorial provides you with a comprehensive understanding of Enformer training. If you have any questions or require further assistance, please don not hesitate to reach out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqbVjem3daVX"
      },
      "source": [
        "The initial line of code utilizes the `pip` package manager to install two Python packages, namely `dm-sonnet` and `tqdm`.\n",
        "\n",
        "1- `dm-sonnet` serves as a deep learning library that is constructed on `TensorFlow`. Its purpose is to simplify the process of constructing and training neural networks by providing a collection of abstractions and modules.\n",
        "\n",
        "2- On the other hand, `tqdm` is a Python library that facilitates the creation of progress bars and the visualization of progress for iterations or tasks within the command line interface.\n",
        "\n",
        "3- The presence of an exclamation mark (i.e., **!**) at the start of the line indicates that the code is being executed within a Jupyter Notebook or an IPython environment. In such environments, the exclamation mark is employed to directly execute shell commands from the notebook. In this particular case, it is utilized to execute the \"pip install\" command for the installation of the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE66P4k-WF7m",
        "outputId": "76bbdeb1-597c-4882-eecf-61e6177df709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.1-py3-none-any.whl (268 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m235.5/268.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (0.1.8)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.22.4)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (0.8.10)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.14.1)\n",
            "Installing collected packages: dm-sonnet\n",
            "Successfully installed dm-sonnet-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dm-sonnet tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ2g59h_d8aI"
      },
      "source": [
        "The `wget` command is utilized to download two Python source code files from specific URLs:\n",
        "\n",
        "1- The first `wget` command downloads a source code file called `attention_module.py` from the GitHub repository: https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/attention_module.py.\n",
        "\n",
        "2- The second `wget` command downloads a source code file named `enformer.py` from the GitHub repository: https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/enformer.py.\n",
        "\n",
        "To run the command quietly without displaying any output or progress information, the `-q` option is used with `wget`. This approach is commonly adopted in scripts or automation tasks to maintain a clean terminal or notebook output without unnecessary clutter.\n",
        "\n",
        "By executing these `wget` commands, the code fetches the Enformer model or module's source code files from the specified URLs. These downloaded files can be subsequently employed locally for further development or incorporated into Python code as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxx1cf42eFrQ"
      },
      "outputs": [],
      "source": [
        "# Get enformer source code\n",
        "!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/attention_module.py\n",
        "!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/enformer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fS5flvBeLcU"
      },
      "source": [
        "**Note**. We comment and explain the `attention_module.py` and `enformer.py` in different Jupyter notebook files in this GitHub repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XjKJ5RVeTm6"
      },
      "source": [
        "**Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDCVtD7DeZpj"
      },
      "source": [
        "The code begins by importing the `TensorFlow` library using the line import `tensorflow` as `tf`.\n",
        "\n",
        "To ensure that a GPU is enabled, it includes an assertion statement. The code verifies the presence of any physical GPU devices by invoking `tf.config.list_physical_devices('GPU')`. If there are no available GPU devices, the code raises an AssertionError with the message `Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU`. This assertion guarantees that the code can leverage the GPU for accelerated computations.\n",
        "\n",
        "Next, the code sets an environment variable named `TF_ENABLE_GPU_GARBAGE_COLLECTION` to false. This variable impacts the garbage collection behavior within TensorFlow when utilizing a GPU. By assigning it a value of false, the code disables GPU-specific garbage collection optimizations. This can be beneficial for simplifying the debugging process, especially when encountering out-of-memory (OOM) errors during GPU computations.\n",
        "\n",
        "In summary, this code snippet ensures the availability of a GPU and adjusts certain TensorFlow settings pertaining to GPU usage and memory management. These adjustments are made for the purpose of facilitating debugging activities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhiDafCgeSDj",
        "outputId": "a0ba551e-4f16-4791-be66-be34ccd7cc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TF_ENABLE_GPU_GARBAGE_COLLECTION=fals\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "# Make sure the GPU is enabled\n",
        "#assert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU'\n",
        "\n",
        "# Easier debugging of OOM\n",
        "%env TF_ENABLE_GPU_GARBAGE_COLLECTION=fals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgU-qzr7ekpV"
      },
      "source": [
        "This code imports several Python libraries and modules:\n",
        "\n",
        "1. `sonnet` from the `snt` module: `Sonnet` is a deep learning library built on top of TensorFlow, and `snt` is a sub-module within Sonnet. It provides additional functionality and abstractions for building neural networks.\n",
        "\n",
        "2. `tqdm`: This library is used for creating progress bars and visualizing the progress of iterations or tasks in the command line interface.\n",
        "\n",
        "3. `IPython.display` from the `clear_output` module: This module provides functionality for controlling the display in IPython environments. `clear_output` is a function that clears the output of the cell or console.\n",
        "\n",
        "4. `numpy` as `np`: NumPy is a fundamental library for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently.\n",
        "\n",
        "5. `pandas` as `pd`: Pandas is a powerful library for data manipulation and analysis. It provides data structures and functions to efficiently work with structured data, such as tables or CSV files.\n",
        "\n",
        "6. `time`: This module provides functions for working with time-related operations, such as measuring elapsed time or introducing delays in the code.\n",
        "\n",
        "7. `os`: The os module provides a way to use operating system-dependent functionality, such as interacting with the file system, working with environment variables, and executing system commands.\n",
        "\n",
        "By importing these libraries and modules, the code gains access to their respective functionalities, allowing for easier and more efficient development of the subsequent code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59V6pHblepf8"
      },
      "outputs": [],
      "source": [
        "import sonnet as snt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU3FcC3DfFdr"
      },
      "source": [
        "This code uses an assert statement to check the version number of the `sonnet` library (`snt`). Specifically, it verifies that the version number starts with `2.0`.\n",
        "\n",
        "The assert statement checks if the condition provided is `True`. If the condition is `False`, it raises an `AssertionError` with an optional error message.\n",
        "\n",
        "In this case, the code asserts that the version number of sonnet starts with `2.0`. If the version number does not meet this condition, an AssertionError will be raised. This assertion is typically used to ensure compatibility or specific features in the code that rely on a particular version of the `sonnet` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pwB_KnEak3B"
      },
      "outputs": [],
      "source": [
        "#assert snt.version.startswith('2.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tib8_gMxfNF3"
      },
      "source": [
        "The code `!nvidia-smi` is used in a Colab notebook or Jupyter notebook to display information about the GPU(s) available in the environment. It executes the shell command `nvidia-smi`, which is a tool provided by `NVIDIA` to monitor and manage NVIDIA GPU devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGQs_rYVfTwr",
        "outputId": "03e0f804-a008-4d92-fe7b-91e4efad9b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# GPU colab has T4 with 16 GiB of memory\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuE7_88BfWM9"
      },
      "source": [
        "**code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9zjExfBfgvL"
      },
      "source": [
        "Importing `enformer` library which is class defined by the author. The class is fully explained in another file in this GitHub page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe8miausudTP"
      },
      "outputs": [],
      "source": [
        "import enformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WvwpAL9fox8"
      },
      "source": [
        "The code defines the `targets_txt` variable, which is a formatted string holding the URL to the target information text file specific to the given organism. The URL is constructed using an f-string, where the value of the organism is inserted into the URL.\n",
        "\n",
        "To read the contents of the target information text file into a DataFrame, the function employs `pd.read_csv()` from the `pandas` library. It assumes that the text file is `tab-separated`, specified by the `sep='\\t'` parameter.\n",
        "\n",
        "The resulting DataFrame, containing the target information, is then returned as the output of the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIcqy0zkfcAO"
      },
      "outputs": [],
      "source": [
        "# @title get_targets(organism)\n",
        "def get_targets(organism):\n",
        "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
        "  return pd.read_csv(targets_txt, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oFrRifjfw47"
      },
      "source": [
        "Here, the author presents a function called `get_dataset`, which serves the purpose of retrieving a dataset for a designated organism and subset. The dataset is acquired from TFRecord files and processed using TensorFlow.\n",
        "\n",
        "Here is the explanation of the code:\n",
        "\n",
        "1. The code begins by defining the `organism_path()` function. This function generates the path to the directory containing the data pertaining to a specific organism.\n",
        "\n",
        "2. The `get_dataset()` function is created, which accepts three arguments: organism (specifying the desired organism), subset (indicating the specific subset of the dataset, such as 'train', 'valid', or 'test'), and `num_threads` (representing the number of parallel threads utilized for reading the `TFRecord` files).\n",
        "\n",
        "3. Within the `get_dataset()` function, a call is made to `get_metadata()`, which retrieves metadata associated with the organism's data. This information may include the count of targets, sequence lengths, and other relevant details.\n",
        "\n",
        "4. The `tfrecord_files()` function is implemented to generate a list of TFRecord file paths corresponding to the specified organism and subset.\n",
        "\n",
        "5. A `TFRecordDataset` object is created and assigned to the dataset variable. This object facilitates the reading of the TFRecord files, utilizing parallel reads and zlib compression.\n",
        "\n",
        "6. The dataset is mapped using the deserialize function. This function performs the deserialization process on the byte-encoded examples found within the TFRecord files, converting them into TensorFlow tensors.\n",
        "\n",
        "7. Finally, the processed dataset is returned as the output of the `get_dataset()` function.\n",
        "\n",
        "In addition to the `get_dataset()` function, several auxiliary functions (`get_metadata()`, `tfrecord_files()`, and `deserialize()`) are defined to assist in obtaining metadata, generating file paths, and deserializing `TFRecord` examples, respectively.\n",
        "\n",
        "Overall, this code snippet enables the retrieval of a dataset for a particular organism and subset from TFRecord files. The dataset can then be further processed and analyzed using TensorFlow's capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAmGX4eZf4iG"
      },
      "outputs": [],
      "source": [
        "# @title get_dataset(organism, subset, num_threads=8)\n",
        "import glob\n",
        "import json\n",
        "import functools\n",
        "\n",
        "\n",
        "def organism_path(organism):\n",
        "  return os.path.join('gs://basenji_barnyard/data', organism)\n",
        "\n",
        "\n",
        "def get_dataset(organism, subset, num_threads=8):\n",
        "  metadata = get_metadata(organism)\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
        "                                    compression_type='ZLIB',\n",
        "                                    num_parallel_reads=num_threads)\n",
        "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
        "                        num_parallel_calls=num_threads)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def get_metadata(organism):\n",
        "  # Keys:\n",
        "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
        "  # pool_width, crop_bp, target_length\n",
        "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
        "  with tf.io.gfile.GFile(path, 'r') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "\n",
        "def tfrecord_files(organism, subset):\n",
        "  # Sort the values by int(*).\n",
        "  return sorted(tf.io.gfile.glob(os.path.join(\n",
        "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
        "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
        "\n",
        "\n",
        "def deserialize(serialized_example, metadata):\n",
        "    \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
        "    feature_map = {\n",
        "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
        "      'target': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "    example = tf.io.parse_example(serialized_example, feature_map)\n",
        "    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
        "    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
        "    sequence = tf.cast(sequence, tf.float32)\n",
        "\n",
        "    target = tf.io.decode_raw(example['target'], tf.float16)\n",
        "    target = tf.reshape(target,\n",
        "                      (metadata['target_length'], metadata['num_targets']))\n",
        "    target = tf.cast(target, tf.float32)\n",
        "\n",
        "    return {'sequence': sequence,\n",
        "          'target': target}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd0cNGMXgNBA"
      },
      "source": [
        "### Load the data set\n",
        "\n",
        "The code snippet is calling the `get_targets()` function to retrieve the target information for the organism `'human'`. It assigns the returned DataFrame to the `variable df_targets_human` and then displays the first few rows using the `head()` method.\n",
        "\n",
        "Assuming the `get_targets()` function is defined properly, it should retrieve the target information for the `'human'` organism from a specific URL. The returned DataFrame, `df_targets_human`, contains the target data, and calling head() on it displays the first few rows of the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JlTR5A2pgWLk",
        "outputId": "39996c90-4bc6-40ea-9e77-df00294b1f83"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>genome</th>\n",
              "      <th>identifier</th>\n",
              "      <th>file</th>\n",
              "      <th>clip</th>\n",
              "      <th>scale</th>\n",
              "      <th>sum_stat</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF833POA</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:cerebellum male adult (27 years) and mal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF110QGM</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:frontal cortex male adult (27 years) and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF880MKD</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:chorion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF463ZLQ</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:Ishikawa treated with 0.02% dimethyl sul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>ENCFF890OGQ</td>\n",
              "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mean</td>\n",
              "      <td>DNASE:GM03348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21d0f6cc-cd5f-4881-9b0f-0f3d0a0d992f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index  genome   identifier  \\\n",
              "0      0       0  ENCFF833POA   \n",
              "1      1       0  ENCFF110QGM   \n",
              "2      2       0  ENCFF880MKD   \n",
              "3      3       0  ENCFF463ZLQ   \n",
              "4      4       0  ENCFF890OGQ   \n",
              "\n",
              "                                                file  clip  scale sum_stat  \\\n",
              "0  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "1  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "2  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "3  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "4  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
              "\n",
              "                                         description  \n",
              "0  DNASE:cerebellum male adult (27 years) and mal...  \n",
              "1  DNASE:frontal cortex male adult (27 years) and...  \n",
              "2                                      DNASE:chorion  \n",
              "3  DNASE:Ishikawa treated with 0.02% dimethyl sul...  \n",
              "4                                      DNASE:GM03348  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_targets_human = get_targets('human')\n",
        "df_targets_human.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PjcefQZghtg"
      },
      "source": [
        "Now the author use code to generates three datasets: `human_dataset`, `mouse_dataset`, and `human_mouse_dataset`. Each dataset is acquired by utilizing the `get_dataset()` function and specifying the organism `('human' or 'mouse')` and subset `('train')`.\n",
        "\n",
        "Here is the explanation of the code:\n",
        "\n",
        "1. The `human_dataset` is obtained by invoking `get_dataset()` with the arguments `'human'` and `'train'`. This call retrieves the dataset for the `'human'` organism and the `'train'` subset.\n",
        "\n",
        "2. Similarly, the `mouse_dataset` is obtained by invoking `get_dataset()` with the arguments `'mouse'` and `'train'`. This call retrieves the dataset for the `'mouse'` organism and the `'train'` subset.\n",
        "\n",
        "3. Both `human_dataset` and `mouse_dataset` are modified by applying the `.batch(1)` method, which groups the elements of the dataset into individual batches, each containing a single element. This approach ensures that each element is processed individually.\n",
        "\n",
        "4. The `.repeat()` method is additionally applied to both datasets. This method repeats the dataset indefinitely, allowing for multiple iterations during training or evaluation.\n",
        "\n",
        "5. The `human_dataset` and `mouse_dataset` are merged using the `tf.data.Dataset.zip()` function, creating a new dataset named `human_mouse_dataset.` This resulting dataset contains pairs of samples, with each sample originating from either the `'human'` or `'mouse'` dataset.\n",
        "\n",
        "6. Lastly, the `.prefetch(2)` method is invoked on `human_mouse_dataset`. This operation prefetches and buffers up to 2 elements, enhancing training performance by overlapping data preprocessing and model training.\n",
        "\n",
        "In summary, this code generates datasets for the `'human'` and `'mouse'` organisms, and then combines them into a single dataset `(human_mouse_dataset)` containing pairs of samples. The resulting dataset is suitable for further processing or training purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG1mCe5SDmHt"
      },
      "outputs": [],
      "source": [
        "human_dataset = get_dataset('human', 'train').batch(1).repeat()\n",
        "mouse_dataset = get_dataset('mouse', 'train').batch(1).repeat()\n",
        "human_mouse_dataset = tf.data.Dataset.zip((human_dataset, mouse_dataset)).prefetch(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdQRwok1gqvM"
      },
      "source": [
        "The code snippet creates an iterator for the `mouse_dataset` using the `iter()` function and assigns it to the variable it. It then retrieves the next element from the iterator using the `next()` function and assigns it to the variable example.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The `iter()` function is called with the mouse_dataset as an argument to create an iterator object. An iterator allows iterating over the elements of a dataset.\n",
        "- The `next()` function is used to retrieve the next element from the iterator it. Each time `next()` is called, it returns the next element of the dataset.\n",
        "- The retrieved element is assigned to the variable example.\n",
        "\n",
        "After executing this code, the example variable will contain the next element from the `mouse_dataset`. You can then use this example to access and manipulate the data within that element for further processing or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhdlJ75EDsKx"
      },
      "outputs": [],
      "source": [
        "it = iter(mouse_dataset)\n",
        "example = next(it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-__0DsTgxhK"
      },
      "source": [
        "This step demonstrates how to iterate over the `human_mouse_dataset` and print information about the elements in each iteration.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The `iter()` function is called with the `human_mouse_dataset` as an argument to create an iterator object, which is assigned to the variable it.\n",
        "- The `next()` function is used to retrieve the next element from the iterator it. Each time the loop iterates, it retrieves the next element of the dataset.\n",
        "- Within the `loop`, a `for loop` is used to iterate over the range of the length of example, which is the number of elements in the current iteration of the dataset.\n",
        "- Inside the for loop, the organism `('human' or 'mouse')` is printed based on the `index i`. It uses a list comprehension `(['human', 'mouse'][i])` to choose the corresponding organism string.\n",
        "- The `example[i]` represents the current element of the dataset for the organism at `index i`. It is a dictionary containing different keys and their corresponding values.\n",
        "- The dictionary items are printed using a dictionary comprehension `{k: (v.shape, v.dtype) for k, v in example[i].items()}`. This displays the shape and data type of each value in the current element.\n",
        "\n",
        "By running this code, it will iterate over the `human_mouse_dataset`, print the organism label `('human' or 'mouse')` for each element, and display the shape and data type information for each value within that element. This can be helpful for understanding the structure and characteristics of the dataset elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeztqJZ74ixT",
        "outputId": "39dc4051-5a19-4443-b6b0-bf6869faf5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "human\n",
            "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 5313]), tf.float32)}\n",
            "mouse\n",
            "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 1643]), tf.float32)}\n"
          ]
        }
      ],
      "source": [
        "# Example input\n",
        "it = iter(human_mouse_dataset)\n",
        "example = next(it)\n",
        "for i in range(len(example)):\n",
        "  print(['human', 'mouse'][i])\n",
        "  print({k: (v.shape, v.dtype) for k,v in example[i].items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvu-Ncc-g3QR"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxWbCZNEg8kL"
      },
      "source": [
        "In this step the author introduces a function called `create_step_function()`, which is responsible for producing and returning a training step function tailored to a given model and optimizer.\n",
        "\n",
        "Here's the explanation of the code:\n",
        "\n",
        "The `create_step_function()` function accepts two arguments: `model` (representing the model utilized for training) and `optimizer` (representing the optimizer employed to update the model's trainable variables).\n",
        "\n",
        "Within the function, a nested function named `train_step()` is defined. This function is decorated with `@tf.function`, enabling it to be compiled and optimized using `TensorFlow's AutoGraph` functionality, leading to improved performance.\n",
        "\n",
        "The `train_step()` function encompasses several parameters, including `batch` (representing the input batch comprising sequences and targets), `head` (signifying a specific output head of the model), and `optimizer_clip_norm_global` (an optional parameter for gradient clipping).\n",
        "\n",
        "Inside the `train_step` function, a gradient tape `(tf.GradientTape)` is employed to record the operations for automatic differentiation. The model's forward pass is executed by invoking model with the input sequences `(batch['sequence'])` while setting `is_training` to `True`. The desired output head `(head)` is extracted from the model's outputs.\n",
        "\n",
        "The `loss` is determined by comparing the predicted outputs `(outputs)` with the target values `(batch['target'])` using `tf.keras.losses.poisson`. The `reduce_mean()` function is utilized to calculate the average `loss` across the batch.\n",
        "\n",
        "To compute the gradients of the loss with respect to the model's trainable variables, the gradient tape `(tape.gradient)` is leveraged.\n",
        "\n",
        "The computed gradients are subsequently applied to the model's trainable variables by invoking the optimizer's apply method, thereby updating the model's parameters.\n",
        "\n",
        "The `loss` is then returned as the output of the `train_step` function.\n",
        "\n",
        "Finally, the `create_step_function()` function returns the `train_step()` function. This `train_step()` function can be utilized to execute a single training step on the model using the specified `optimizer`. It offers a convenient and efficient approach to train the model by encapsulating the necessary operations within a TensorFlow function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgPWA-GahAx7"
      },
      "outputs": [],
      "source": [
        "def create_step_function(model, optimizer):\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(batch, head, optimizer_clip_norm_global=0.2):\n",
        "    with tf.GradientTape() as tape:\n",
        "      outputs = model(batch['sequence'], is_training=True)[head]\n",
        "      loss = tf.reduce_mean(\n",
        "          tf.keras.losses.poisson(batch['target'], outputs))\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply(gradients, model.trainable_variables)\n",
        "\n",
        "    return loss\n",
        "  return train_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qOJA8kGhImO"
      },
      "source": [
        "The code snippet sets up the `learning rate`, `optimizer`, `model`, and `training step()` function for training a model using the `Enformer` architecture.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- A `learning_rate` variable is created using `tf.Variable` and initialized to 0. It is set as non-trainable by setting `trainable=False`. This variable is used to control the learning rate during training.\n",
        "- An `Adam optimizer` is created using `snt.optimizers.Adam`, and the `learning_rate` is set to the previously created variable `learning_rate`.\n",
        "- The `num_warmup_steps` variable is set to `5000`, which represents the number of `warm-up` steps for the learning rate schedule.\n",
        "- The `target_learning_rate` is set to `0.0005`, which represents the desired learning rate after the warm-up period.\n",
        "- An instance of the Enformer model is created with specific configurations, such as the number of channels, number of heads, number of transformer layers, and pooling type.\n",
        "- The `create_step_function()` function is called with the` model `and `optimizer` as arguments, and the returned `train_step` function is assigned to the `train_step()` variable.\n",
        "\n",
        "After executing this code, you can use the `train_step()` function to perform a single training step on the model using the specified optimizer. The learning rate can be controlled by adjusting the value of the `learning_rate` variable. This setup provides the necessary components for training the `Enformer` model with the specified configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXOa1rtBhM9k"
      },
      "outputs": [],
      "source": [
        "learning_rate = tf.Variable(0., trainable=False, name='learning_rate')\n",
        "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
        "num_warmup_steps = 5000\n",
        "target_learning_rate = 0.0005\n",
        "\n",
        "model = enformer.Enformer(channels=1536 // 4,  # Use 4x fewer channels to train faster.\n",
        "                          num_heads=8,\n",
        "                          num_transformer_layers=11,\n",
        "                          pooling_type='max')\n",
        "\n",
        "train_step = create_step_function(model, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsKKC6chhRl3"
      },
      "source": [
        "In the following, the code trains the model based on the specified configuration and outputs the loss and learning rate at the conclusion of each epoch. You can see the loss for `human` and `mouse` in this part, that would worth also if we consider learning rate also in our calculation and regards its effects on our results.\n",
        "\n",
        "Here's the explanation of the code:\n",
        "\n",
        "1. The `steps_per_epoch` variable is initialized to `20`, indicating the number of steps to iterate through the dataset in each epoch.\n",
        "\n",
        "2. The `num_epochs` variable is set to `5`, representing the total number of epochs during which the model will be trained.\n",
        "\n",
        "3. The `data_it` variable is created by invoking `iter(human_mouse_dataset)`, generating an iterator for the `human_mouse_dataset`.\n",
        "\n",
        "4. A global step counter, `global_step`, is initialized to `0`. This variable keeps track of the overall number of training steps across all epochs.\n",
        "\n",
        "5. The training process is executed using nested loops. The outer loop iterates over the range of `num_epochs`, while the inner loop iterates over the range of `steps_per_epoch`.\n",
        "\n",
        "6. Within the inner loop, the `global_step` is incremented by 1 to monitor the progress.\n",
        "\n",
        "7. Following the initial training step `(global_step > 1)`, the `learning rate` is adjusted based on the current global step and the number of warm-up steps. The learning rate fraction is computed by dividing the current global step by the `maximum of 1` and the number of warm-up steps. The `learning rate` is subsequently updated by assigning the target learning rate multiplied by the learning rate fraction to the learning_rate variable.\n",
        "\n",
        "8. The `next(data_it)` function is employed to retrieve the subsequent batch of data from the iterator. This function returns a tuple of batches, with each batch corresponding to the `'human'` and `'mouse'` `organisms`, respectively.\n",
        "\n",
        "9. The `train_step()` function is invoked twice to execute the training step on both the `'human'` and `'mouse'` batches individually. The losses `(loss_human and loss_mouse)` are obtained.\n",
        "\n",
        "10. At the conclusion of each epoch, the loss values for both organisms and the current learning rate are printed.\n",
        "\n",
        "By executing this code, the model undergoes training for the specified number of epochs, and the `loss` and `learning rate` are displayed at the conclusion of each epoch. The training process entails iterating over the dataset, updating the model parameters, and adjusting the learning rate based on the global step and warm-up schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrbDaOMWcFUl",
        "outputId": "6a42f69c-3003-47f2-a8d2-1b94c52eb57e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:24<00:00,  1.25s/it]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.774059 loss_mouse 0.94303024 learning_rate 2.0000002e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.0067647 loss_mouse 0.8752468 learning_rate 4.0000004e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.0471998 loss_mouse 0.89318746 learning_rate 6e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.010262 loss_mouse 1.02991 learning_rate 8.000001e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "loss_human 1.111991 loss_mouse 0.84773445 learning_rate 1.0000001e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "steps_per_epoch = 20\n",
        "num_epochs = 5\n",
        "\n",
        "data_it = iter(human_mouse_dataset)\n",
        "global_step = 0\n",
        "for epoch_i in range(num_epochs):\n",
        "  for i in tqdm(range(steps_per_epoch)):\n",
        "    global_step += 1\n",
        "\n",
        "    if global_step > 1:\n",
        "      learning_rate_frac = tf.math.minimum(\n",
        "          1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))\n",
        "      learning_rate.assign(target_learning_rate * learning_rate_frac)\n",
        "\n",
        "    batch_human, batch_mouse = next(data_it)\n",
        "\n",
        "    loss_human = train_step(batch=batch_human, head='human')\n",
        "    loss_mouse = train_step(batch=batch_mouse, head='mouse')\n",
        "\n",
        "  # End of epoch.\n",
        "  print('')\n",
        "  print('loss_human', loss_human.numpy(),\n",
        "        'loss_mouse', loss_mouse.numpy(),\n",
        "        'learning_rate', optimizer.learning_rate.numpy()\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ME8bBihdC8"
      },
      "source": [
        "**Evaluation (pearsonR)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdn4vUrhpf8"
      },
      "source": [
        "In this part, the code snippet defines a function `evaluate_model()` that evaluates the performance of a model on a given dataset and a specific output head.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The `evaluate_model()` function takes three arguments: `model` (the model to be evaluated), `dataset` (the dataset on which evaluation will be performed), and `head` (the specific output head of the model).\n",
        "- Inside the function, a `MetricDict` object is created with the initial metric of `'PearsonR'`. This object is used to store and compute evaluation metrics.\n",
        "- A nested function predict is defined using the `@tf.function` decorator. This function takes an `input x` and returns the predicted outputs of the model for the specified `head`. It sets `is_training` to `False` to ensure evaluation mode.\n",
        "- A loop is performed over the dataset using `enumerate(dataset)`. The loop iterates over the batches of the dataset, and the index is stored in i and the batch data is stored in batch.\n",
        "- If a `max_steps` value is provided and the current iteration exceeds that `value`, the `loop` breaks.\n",
        "- Inside the loop, the `metric.update_state` method is called to update the metric with the ground truth targets `(batch['target'])` and the predicted outputs obtained by calling the predict function on the input sequences `(batch['sequence'])`.\n",
        "- After iterating over the dataset, the evaluation result is obtained by calling `metric.result()`, which returns the computed evaluation metric.\n",
        "\n",
        "The `evaluate_model()` function allows for evaluating the performance of a model on a given dataset and specific output head. It computes the specified evaluation metric by comparing the model's predictions with the ground truth targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJVHFAzehtCO"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset, head, max_steps=None):\n",
        "  metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
        "  @tf.function\n",
        "  def predict(x):\n",
        "    return model(x, is_training=False)[head]\n",
        "\n",
        "  for i, batch in tqdm(enumerate(dataset)):\n",
        "    if max_steps is not None and i > max_steps:\n",
        "      break\n",
        "    metric.update_state(batch['target'], predict(batch['sequence']))\n",
        "\n",
        "  return metric.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrZs6ziHhwGM"
      },
      "source": [
        "Now the code evaluates the model's performance on the `'human'` dataset using the `'human'` output head and computes the mean value of the evaluation metrics.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The `evaluate_model()` function is called with the following arguments:\n",
        "  - `model`: The model to be evaluated.\n",
        "  - `dataset`: The dataset for evaluation, obtained by calling `get_dataset('human', 'valid').batch(1).prefetch(2)`. It batches the data with a batch size of 1 and prefetches 2 batches for improved performance.\n",
        "  - `head`: The specific output head of the model to evaluate, which is set to 'human'.\n",
        "  - `max_steps`: The maximum number of steps to perform evaluation, which is set to `100`.\n",
        "- The evaluation metrics for the `'human'` dataset are stored in the `metrics_human` variable.\n",
        "- The computed mean values of the evaluation metrics are printed using a dictionary comprehension, where the key is the metric name and the value is the mean value obtained by calling `.numpy().mean()` on each metric.\n",
        "\n",
        "By running this code, the model will be evaluated on the 'human' dataset using the specified output head, and the mean values of the evaluation metrics will be displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMylwJteEjE0"
      },
      "outputs": [],
      "source": [
        "metrics_human = evaluate_model(model,\n",
        "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
        "                               head='human',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_human.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as0vB4XbiDsO"
      },
      "source": [
        "Also the code snippet evaluates the model's performance on the `'mouse'` dataset using the `'mouse'` output head and computes the mean value of the evaluation metrics.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "- The `evaluate_model()` function is called with the following arguments:\n",
        "  - `model`: The model to be evaluated.\n",
        "  - `dataset`: The dataset for evaluation, obtained by `calling get_dataset('mouse', 'valid').batch(1).prefetch(2)`. It batches the data with a batch size of 1 and prefetches 2 batches for improved performance.\n",
        "  - `head`: The specific output head of the model to evaluate, which is set to `'mouse'`.\n",
        "  -` max_steps`: The maximum number of steps to perform evaluation, which is set to` 100`.\n",
        "- The evaluation metrics for the `'mouse'` dataset are stored in the `metrics_mouse` variable.\n",
        "- The computed mean values of the evaluation metrics are printed using a dictionary comprehension, where the key is the metric name and the value is the mean value obtained by calling `.numpy().mean()` on each metric.\n",
        "\n",
        "By running this code, the model will be evaluated on the `'mouse'` dataset using the specified output head, and the mean values of the evaluation metrics will be displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY_wj95xiDtE",
        "outputId": "fea839f7-b6c9-46ed-aece-c56b02e9ea16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:21,  6.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{'PearsonR': 0.005183698}\n"
          ]
        }
      ],
      "source": [
        "metrics_mouse = evaluate_model(model,\n",
        "                               dataset=get_dataset('mouse', 'valid').batch(1).prefetch(2),\n",
        "                               head='mouse',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_mouse.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySIZrf6i-Qc7"
      },
      "source": [
        "Similarly, lets see the results also on `human` when we run the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57fNitK9hzwd",
        "outputId": "947aaadb-dad2-4a00-ddac-d765f65d782f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:23,  6.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "{'PearsonR': 0.0028573992}\n"
          ]
        }
      ],
      "source": [
        "metrics_human = evaluate_model(model,\n",
        "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
        "                               head='human',\n",
        "                               max_steps=100)\n",
        "print('')\n",
        "print({k: v.numpy().mean() for k, v in metrics_human.items()})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}